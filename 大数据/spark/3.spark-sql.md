# 一、Spark SQL 概念

## 1. 什么是 Spark SQL

​		Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块.与基本的 Spark RDD API 不同, Spark SQL 的抽象数据类型为 Spark 提供了关于数据结构和正在执行的计算的更多信息.在内部, Spark SQL 使用这些额外的信息去做一些额外的优化.

​		有多种方式与 Spark SQL 进行交互, 比如: SQL 和 Dataset API. 当计算结果的时候, 使用的是相同的执行引擎, 不依赖你正在使用哪种 API 或者语言.这种统一也就意味着开发者可以很容易在不同的 API 之间进行切换, 这些 API 提供了最自然的方式来表达给定的转换.

​		和 Hive相比，它是将 Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写 MapReduc 的程序的复杂性，由于 MapReduce 这种计算模型执行效率比较慢, 所以 Spark SQL 的应运而生，它是将 Spark SQL 转换成 RDD，然后提交到集群执行，执行效率非常快！

Spark SQL 它提供了2个编程抽象, 类似 Spark Core 中的 **RDD**

- 1. **DataFrame**

- 2.  **DataSet**



## 2. Spark SQL 特点

- **Integrated(易整合)**：无缝的整合了 SQL 查询和 Spark 编程.

- **Uniform Data Access(统一的数据访问方式)：**使用相同的方式连接不同的数据源.

- **Hive Integration(集成 Hive)**：在已有的仓库上直接运行 SQL 或者 HiveQL

- **Standard Connectivity(标准的连接方式)**：通过 JDBC 或者 ODBC 来连接



## 3. **DataFrame**

​		与 RDD 类似，**DataFrame** 也是一个分布式数据容器。然而**DataFrame**更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即**schema**。

​		同时，与**Hive**类似，**DataFrame**也支持嵌套数据类型（**struct**、**array**和**map**）。从 API 易用性的角度上看，**DataFrame API**提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。

![](https://i.bmp.ovh/imgs/2021/04/529c9772c2df32c3.png)





上图直观地体现了**DataFrame**和**RDD**的区别。

​		左侧的**RDD[Person]**虽然以**Person**为类型参数，但Spark框架本身不了解**Person**类的内部结构。而右侧的**DataFrame**却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。

**DataFrame**是为数据提供了**Schema**的视图。可以把它当做数据库中的一张表来对待，**DataFrame**也是懒执行的

性能上比 **RDD**要高，主要原因： 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。





## 4. DataSet

1. 是**DataFrame API**的一个扩展，是 SparkSQL 最新的数据抽象(1.6新增)。

2. 用户友好的API风格，既具有类型安全检查也具有**DataFrame**的查询优化特性。

3.  **Dataset**支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。

4. 样例类被用来在**DataSet**中定义数据的结构信息，样例类中每个属性的名称直接映射到**DataSet**中的字段名称。

5. **DataFrame**是**DataSet**的特列，**DataFrame=DataSet[Row]** ，所以可以通过**as**方法将**DataFrame**转换为**DataSet**。**Row**是一个类型，跟**Car、Person**这些的类型一样，所有的表结构信息都用**Row**来表示。

6. **DataSet**是强类型的。比如可以有**DataSet[Car]**，**DataSet[Person]**.

7. **DataFrame**只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个**String**进行减法操作，在执行的时候才报错，而**DataSet**不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟**JSON**对象和类对象之间的类比。



# 二、Spark SQL 编程

## 1.SparkSession

​			在老的版本中，SparkSQL 提供两种 SQL 查询起始点：一个叫**SQLContext**，用于Spark 自己提供的 SQL 查询；一个叫 **HiveContext**，用于连接 Hive 的查询。

​			从2.0开始, **SparkSession**是 Spark 最新的 SQL 查询起始点，实质上是**SQLContext**和**HiveContext**的组合，所以在**SQLContext**和**HiveContext**上可用的 API 在**SparkSession**上同样是可以使用的。

**SparkSession**内部封装了**SparkContext**，所以计算实际上是由**SparkContext**完成的。当我们使用 spark-shell 的时候, spark 会自动的创建一个叫做**spark**的**SparkSession**, 就像我们以前可以自动获取到一个**sc**来表示**SparkContext**





## 2. 使用DataFrame编程

Spark SQL 的 **DataFrame** API 允许我们使用 **DataFrame** 而不用必须去注册临时表或者生成 SQL 表达式.

**DataFrame** API 既有 **transformation**操作也有**action**操作. **DataFrame**的转换从本质上来说更具有关系, 而 **DataSet** API 提供了更加函数式的 API

ds 功能相比df功能更加强大，不过我们常使用的是df(DataFrame)

### 2.1.创建DataFrame

- 1.1 通过 Spark 的数据源创建：jdbc,json,parquet,hive,scala集合

- 1.2 通过已知的 **RDD** 来创建

- 1.3 通过查询一个 Hive 表来创建.



例子：

```shell
scala> val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df show
warning: there was one feature warning; re-run with -feature for details
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+


scala> 
```



### 2.2.DataFrame的 shell操作

- sql风格
- dsl风格

#### 2.2.1 sql 风格

创建临时表

```bash
df.createOrReplaceTempView (创建或者替换原来的表)

df.createTempView  （创建表）

df.createGlobalTempView (创建全局表，上面的创建只是当前session，这里是全局临时视图表)
```





```shell
scala> val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df show
warning: there was one feature warning; re-run with -feature for details
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+

scala> df.createTempView("emp")

scala> val df1 = spark.sql("select * from emp")
df1: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df1.show
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+

```



#### 2.2.2 dsl 风格

**DataFrame**提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据. 可以在 Scala, Java, Python 和 R 中使用 DSL

使用 DSL 语法风格不必去创建临时视图了.



使用select $ 等来实现，不过常用的还是sql方式

```bash
scala>  val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df.printSchema
root
 |-- name: string (nullable = true)
 |-- salary: long (nullable = true)


scala> df.select($"name",$"salary").show
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+


scala> df.select($"name",$"salary"+100).show
+-------+--------------+
|   name|(salary + 100)|
+-------+--------------+
|Michael|          3100|
|   Andy|          4600|
| Justin|          3600|
|  Berta|          4100|
+-------+--------------+


scala> df.select("name").show
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
|  Berta|
+-------+

scala> df.filter($"salary">4000).show
+----+------+
|name|salary|
+----+------+
|Andy|  4500|
+----+------+

```



## 3 .DF 的代码操作

第一步添加spark-sql的依赖

```xml
<dependency>
     <groupId>org.apache.spark</groupId>
     <artifactId>spark-sql_2.11</artifactId>
     <version>2.1.1</version>
</dependency>
```

#### 3.1 创建DF

```scala
object CreateDf {

  def main(args: Array[String]): Unit = {

    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()

    //2. 通过sparkSession创建DF
    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")

    //3.对DF做操作（sql）
    df.createOrReplaceTempView("user")
    sparkSession.sql("select * from user").show()

    //4.创建sparkSession
    sparkSession.stop()


  }

}
```

#### 3.2 RDD 转DF

```scala
object RDD2DF {

  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()
    val sc = sparkSession.sparkContext
	// 这里需要引入隐式转换
    import sparkSession.implicits._
	//1.通过元组来转换
    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22)::Nil)
    rdd.toDF("name","age").show

	// 2.样例类定义了表结构: 样例类参数名通过反射被读到, 然后成为列名.
    val rdd1 = sc.parallelize(Array(User("hh", 22), User("ss", 11)))
    rdd1.toDF("name","age").show

    sparkSession.stop

  }

}

case class User(name:String,age:Int)
```



#### 3.3 DF 转RDD

```scala
object DF2RDD {

  def main(args: Array[String]): Unit = {

    //1.创建sparkSession
    val spark = SparkSession.builder()
      .appName("createDf")
      .master("local[2]")
      .getOrCreate()

    import spark.implicits._

      //确定是toDF无法确定转换后的数据类型
    val df = (1 to 5).toDF("number")
    df.show()

    // rdd 转为 ,返回的是Row类型
    val rdd: RDD[Row] = df.rdd
    rdd.collect().map(row => row.getInt(0)).foreach(println)

    spark.stop()

  }

}
```



通过api来实现，这里可以确定数据的类型

```scala
object RDD2DF2 {

  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()
    val sc = sparkSession.sparkContext

    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22) :: Nil)
      .map {
        case (name, age) => Row(name, age)
      }

    val schema = StructType(Array(StructField("name", StringType),
      StructField("age", IntegerType)))
    val df = sparkSession.createDataFrame(rdd, schema)
    df.show


    sparkSession.stop

  }


}
```



## 4.使用DataSet 进行编程

**DataSet** 和 **RDD** 类似, 但是**DataSet**没有使用 Java 序列化或者 Kryo序列化, 而是使用一种专门的编码器去序列化对象, 然后在网络上处理或者传输.

虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许Spark执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。

**DataSet**是具有**强类型的数据集合**，需要提供对应的类型信息。



```scala
package com.vison.sparksql

import org.apache.spark.sql.{Dataset, SparkSession}

object CreateDS {

  def main(args: Array[String]): Unit = {
    val sparkSession = SparkSession.builder()
      .appName("createDf")
      .master("local[2]")
      .getOrCreate()
    import  sparkSession.implicits._
    val sc = sparkSession.sparkContext
    val list1 = List(10,203,30,20,12,111,2333,22)
    //df能用的ds一定能用的，df只是ds的一种特殊方式

    //1.通过scala的序列得到
     val ds:Dataset[Int] = list1.toDS
    ds.createTempView("tem")
    sparkSession.sql("select * from tem").show


    //2.rdd ds 相互转换得到
    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22)::Nil)
    val ds1 = rdd.toDS()
    ds1.show()

    ds1.rdd.collect().foreach(println) //ds转rdd

    //3.通过ds 和 df想换转换

    //df -> ds
    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")
    val ds2:Dataset[Employee] = df.as[Employee]
    ds2.show

    //df -> ds
    val df2 = ds2.toDF()
    df2.show
    sparkSession.stop
  }

}
case class Employee(name:String,salary:Long)
```



## 5. RDD,DF,DS 的联系

三者间的相互转换

![](https://i.bmp.ovh/imgs/2021/04/41df5b79a78a3703.png)

在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是**DataFrame**和**DataSet**。他们和RDD有什么区别呢？首先从版本的产生上来看：

RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6)

如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。

在后期的 Spark 版本中，**DataSet**会逐步取代**RDD**和**DataFrame**成为唯一的 API 接口。

### 5.1 共性

- **RDD**、**DataFrame**、**Dataset**全都是 Spark 平台下的分布式弹性数据集，为处理超大型数据提供便利

- 三者都有惰性机制，在进行创建、转换，如**map**方法时，不会立即执行，只有在遇到**Action**如**foreach**时，三者才会开始遍历运算。

- 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出

- 三者都有**partition**的概念

- 三者有许多共同的函数，如**map, filter**，排序等

- 在对 **DataFrame**和**Dataset**进行操作许多操作都需要这个包进行支持 **import spark.implicits._**

- **DataFrame**和**Dataset**均可使用模式匹配获取各个字段的值和类型

### 5.2 区别

##### 5.2.1 **RDD**

*1.* **RDD**一般和**spark mlib**同时使用

*2.* **RDD**不支持**sparksql**操作

##### *5.2.2* **DataFrame**

1. 与**RDD**和**Dataset**不同，**DataFrame**每一行的类型固定为**Row**，每一列的值没法直接访问，只有通过解析才能获取各个字段的值，

2. **DataFrame**与**DataSet**一般不与 spark mlib 同时使用

3. **DataFrame**与**DataSet**均支持 SparkSQL 的操作，比如**select，groupby**之类，还能注册临时表/视窗，进行 sql 语句操作

4. **DataFrame**与**DataSet**支持一些特别方便的保存方式，比如保存成**csv**，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)

##### *5.2.3* **DataSet**

1. **Dataset**和**DataFrame**拥有完全相同的成员函数，区别只是每一行的数据类型不同。 **DataFrame**其实就是**DataSet**的一个特例

2. **DataFrame**也可以叫**Dataset[Row]**,每一行的类型是**Row**，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的**getAS**方法或者共性中的第七条提到的模式匹配拿出特定字段。而**Dataset**中，每一行是什么类型是不一定的，在自定义了**case class**之后可以很自由的获得每一行的信息



## 6.自定义聚合参数

强类型的**Dataset**和弱类型的**DataFrame**都提供了相关的聚合函数， 如 **count()，countDistinct()，avg()，max()，min()**。除此之外，用户可以设定自己的自定义聚合函数

继承**UserDefinedAggregateFunction**

### 6.1 弱类型的聚合函数

```scala
package com.vison.sparksql

import org.apache.spark.sql.{Row, SparkSession, types}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DoubleType, IntegerType, LongType, StructField, StructType}
/**
 * {"name":"Michael", "salary":3000}
 * {"name":"Andy", "salary":4500}
 * {"name":"Justin", "salary":3500}
 * {"name":"Berta", "salary":4000}
 */
object UDAFDemo {


  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder()
      .appName("createDf").master("local[2]")
      .getOrCreate()

    import  sparkSession.implicits._
    val sc = sparkSession.sparkContext

    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")
    df.createOrReplaceTempView("user")
    //注册聚合函数 sum
    sparkSession.udf.register("mySum",new MySum)
    sparkSession.sql("select mySum(salary) from user").show

    sparkSession.udf.register("myAvg",new MyAvg)
    sparkSession.sql("select myAvg(salary) from user").show


    sparkSession.close()

  }

}

/**
 * 自定义聚合函数 求和
 */
//StructType 表示数据类型
class MySum extends UserDefinedAggregateFunction{
  //输入的数据类型
  override def inputSchema: StructType = StructType(StructField("ele",IntegerType)::Nil)
  //缓冲区的类型
  override def bufferSchema: StructType = StructType(StructField("sum",IntegerType)::Nil)
  // 最终聚合结果的类型
  override def dataType: DataType = IntegerType

  //相同的输入是否返回相同的输出，一般都是true
  override def deterministic: Boolean = true
  //对缓冲区初始化
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0
  }

  //分区类聚合
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
     // input 使用聚合函数的时，传过来的参数封装为row
    if (!input.isNullAt(0)){ //避免age为null
      val d = input.getInt(0)
      buffer(0) = buffer.getInt(0) + d
    }

  }

  //分区间的聚合,
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    //把buffer1和buffer2聚合在一起，然后值写回到buffer1
    buffer1(0) = buffer1.getInt(0) + buffer2.getInt(0)
  }

  //返回最终的输出值
  override def evaluate(buffer: Row): Any = buffer.getInt(0)
}

/**
 * 自定义聚合函数 求平均值
 */
//StructType 表示数据类型
class MyAvg extends UserDefinedAggregateFunction{
  //输入的数据类型
  override def inputSchema: StructType = StructType(StructField("ele",IntegerType)::Nil)
  //缓冲区的类型
  override def bufferSchema: StructType = StructType(StructField("sum",IntegerType):: StructField("count",LongType)::Nil)
  // 最终聚合结果的类型
  override def dataType: DataType = DoubleType

  //相同的输入是否返回相同的输出，一般都是true
  override def deterministic: Boolean = true
  //对缓冲区初始化
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0
    buffer(1) = 0L
  }

  //分区类聚合
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    // input 使用聚合函数的时，传过来的参数封装为row
    if (!input.isNullAt(0)){ //避免age为null
      buffer(0) = buffer.getInt(0) + input.getInt(0)
      buffer(1) = buffer.getLong(1) + 1L
    }

  }

  //分区间的聚合,
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    //把buffer1和buffer2聚合在一起，然后值写回到buffer1
    buffer1(0) = buffer1.getInt(0) + buffer2.getInt(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }

  //返回最终的输出值
  override def evaluate(buffer: Row): Any = buffer.getInt(0).doubleValue() / buffer.getLong(1).doubleValue()
}


```



### 6.2 强类型的聚合函数

DataSet

```scala
package com.vison.sparksql

import org.apache.spark.sql.{Dataset, Encoder, Encoders, Row, SparkSession, types}
import org.apache.spark.sql.expressions.{Aggregator, MutableAggregationBuffer, UserDefinedAggregateFunction}
/**
 * {"name":"Michael", "salary":3000}
 * {"name":"Andy", "salary":4500}
 * {"name":"Justin", "salary":3500}
 * {"name":"Berta", "salary":4000}
 */
object UDAFDemo2 {


  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder()
      .appName("createDf").master("local[2]")
      .getOrCreate()

    import  sparkSession.implicits._
    val sc = sparkSession.sparkContext

    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")

    val ds:Dataset[Person]  = df.as[Person]

    val avg = new MyAvg2().toColumn.name("avg")
    val result = ds.select(avg)
    result.show

    sparkSession.close()

  }

}

case class Person(name:String,salary:Long)
case class SalaryAvg(sum:Long,count:Long){
  def avg: Double = {
    println(sum+"==="+count)
    sum.doubleValue()/count.doubleValue()
  }
}
/**
 * 自定义聚合函数 求平均值
 */                             //in,  buffer , out
class MyAvg2 extends Aggregator[Person,SalaryAvg,Double]{
  //缓冲区初始化
  override def zero: SalaryAvg = SalaryAvg(0,0)
  //分区内聚合
  override def reduce(b: SalaryAvg, a: Person): SalaryAvg = a match {
    case Person(name,salary) => SalaryAvg(b.sum+salary,b.count+1L)
    case _ => b
  }
  //分区间聚合
  override def merge(b1: SalaryAvg, b2: SalaryAvg): SalaryAvg = {
    SalaryAvg(b1.sum+b2.sum,b1.count+b2.count)
  }
  //最终返回值
  override def finish(reduction: SalaryAvg): Double = reduction.avg
  //buffer编码
  override def bufferEncoder: Encoder[SalaryAvg] = Encoders.product
  //输出结果编码
  override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}
```





## 7.SparkSQL 数据源

Spark SQL 的**DataFrame**接口支持操作多种数据源. 一个 **DataFrame**类型的对象可以像 **RDD** 那样操作(比如各种转换), 也可以用来创建临时表.



### 7.1 读

默认数据源是**parquet**, 我们也可以通过使用:**spark.sql.sources.default**这个属性来设置默认的数据源.

```scala
val usersDF = spark.read.load("examples/src/main/resources/users.parquet")
//指定格式读，可以json/csv等
spark.read.format("json").load("examples/src/main/resources/people.json")

spark.sql("select * from json.`examples/src/main/resources/people.json`")
```



```scala
// 等价于spark.read.format("json").load(）
spark.read.json("examples/src/main/resources/people.json")
```



### 7.2 写

可以手动给数据源指定一些额外的选项. 数据源应该用全名称来指定, 但是对一些内置的数据源也可以使用短名称:**json, parquet, jdbc, orc, libsvm, csv, text**

```scala

df.write.save()
//写为json格式
df.write.format("json").save()

df.write.format("json").mode("error").save()
```



#### 1)文件保存选项(SaveMode)

保存操作可以使用 SaveMode, 用来指明如何处理数据. 使用**mode()**方法来设置.

有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作. 还有, 如果你执行的是 Overwrite 操作, 在写入新的数据之前会先删除旧的数据.

相关选项：

| Scala/Java                          | Any Language         | Meaning                    |
| ----------------------------------- | -------------------- | -------------------------- |
| **SaveMode.ErrorIfExists**(default) | **"error"**(default) | 如果文件已经存在则抛出异常 |
| **SaveMode.Append**                 | **"append"**         | 如果文件已经存在则追加     |
| **SaveMode.Overwrite**              | **"overwrite"**      | 如果文件已经存在则覆盖     |
| **SaveMode.Ignore**                 | **"ignore"**         | 如果文件已经存在则忽略     |



### 7.3 jdbc 操作

Spark SQL 也支持使用 JDBC 从其他的数据库中读取数据. JDBC 数据源比使用 **JdbcRDD**更爽一些. 这是因为返回的结果直接就是一个 **DataFrame**, **DataFrame**更加容易被处理或者与其他的数据源进行 **join**.

Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 **DataFrame**，通过对**DataFrame**一系列的计算后，还可以将数据再写回关系型数据库中。

```scala
package com.vison.rw

import java.util.Properties

import org.apache.spark.sql.SparkSession

object JDBCRead {

  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder()
      .appName("createDf").master("local[2]")
      .getOrCreate()

    import sparkSession.implicits._
    val sc = sparkSession.sparkContext

    //1读取jdbc,通用的
    var df = sparkSession.read.option("url", "jdbc:mysql://hadoop101:3306/test")
      .option("user", "root")  //用户名
      .option("password", "root")
      .option("dbtable", "user") //读取的表
      .format("jdbc")
      .load()

    df.show

    //2专用读
    val props = new Properties()
    props.setProperty("user","root")
    props.setProperty("password","root")
    val df1 = sparkSession.read.jdbc("jdbc:mysql://hadoop101:3306/test","user",props)

    df1.show


    //3.通用jdbc 写

    df = sparkSession.read.json("c://user.json");
    df.write.format("jdbc")
      .option("user", "root")  //用户名
      .option("password", "root")
      .option("dbtable", "user") //写入的表
      .format("jdbc")
      .mode("append") //追加写入，避免表存在报错
      .save

    //4.专用的写
    df.write.jdbc("jdbc:mysql://hadoop101:3306/test","user",props)
    sparkSession.close()
  }

}

```





### 7.4 hive

Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQ L编译时可以包含 Hive 支持，也可以不包含。

包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。

若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。



#### 1) 使用内置hive

在实际使用中, 几乎没有任何人会使用内置的 Hive

如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.Hive 的元数据存储在 derby 中, 仓库地址:**$SPARK_HOME/spark-warehouse**

```scala
scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+


scala> spark.sql("create table aa(id int)")
21/02/09 18:36:10 WARN HiveMetaStore: Location: file:/opt/module/spark-local/spark-warehouse/aa specified for non-external table:aa
res2: org.apache.spark.sql.DataFrame = []

scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|       aa|      false|
+--------+---------+-----------+
```

向表中加载本地数据数据

```scala
scala> spark.sql("load data local inpath './ids.txt' into table aa")
res8: org.apache.spark.sql.DataFrame = []

scala> spark.sql("select * from aa").show
+---+
| id|
+---+
|100|
|101|
|102|
|103|
|104|
|105|
|106|
```



#### 2)  使用外置Hive

一般我们有 spark on hive. 还有hive on spark(这个是hive提供，hive和spark的版本兼容很严格) ，



spark on hive 一般只需要找到hive的元数据就可以了。



**准备工作**

注意spark和hive的版本，spark2.x 对应的hive1.2.x等。

- Spark 要接管 Hive 需要把 **hive-site.xml** copy 到**conf/**目录下(我们这里的metadata数据是存储到mysql中的).

- 把 Mysql 的驱动 copy 到 **jars/**目录下.

- 如果访问不到**hdfs**, 则需要把**core-site.xml**和**hdfs-site.xml** 拷贝到**conf/**目录下.



**启动 spark-shell**

```scala

scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|      emp|      false|
+--------+---------+-----------+
scala> spark.sql("select * from emp").show
19/02/09 19:40:28 WARN LazyStruct: Extra bytes detected at the end of the row! Ignoring similar problems.
+-----+-------+---------+----+----------+------+------+------+
|empno|  ename|      job| mgr|  hiredate|   sal|  comm|deptno|
+-----+-------+---------+----+----------+------+------+------+
| 7369|  SMITH|    CLERK|7902|1980-12-17| 800.0|  null|    20|
| 7499|  ALLEN| SALESMAN|7698| 1981-2-20|1600.0| 300.0|    30|
| 7521|   WARD| SALESMAN|7698| 1981-2-22|1250.0| 500.0|    30|
| 7566|  JONES|  MANAGER|7839|  1981-4-2|2975.0|  null|    20|
| 7654| MARTIN| SALESMAN|7698| 1981-9-28|1250.0|1400.0|    30|
+-----+-------+---------+----+----------+------+------+------+
```



在**spark-shell**执行 hive 方面的查询比较麻烦.**spark.sql("").show**

Spark 专门给我们提供了书写 HiveQL 的工具: **spark-sql** cli



**使用hiveserver2 + beeline**

spark-sql 得到的结果不够友好, 所以可以使用**hiveserver2 + beeline**

```scala
1.启动 thrift服务器
sbin/start-thriftserver.sh \
--master yarn \
--hiveconf hive.server2.thrift.bind.host=node1 \
-–hiveconf hive.server2.thrift.port=10000 \


2.启动beeline客户端
bin/beeline

# 然后输入
!connect jdbc:hive2://node1:10000
# 然后按照提示输入用户名和密码
```





yarn 模式下操作

```sh
bin/spark-shell --master yarn
bin/spark-sql --master yarn

## 可以当成hive使用,类似执行数仓脚本，如下
bin/spark-sql --master yarn -e "show databases"
```



#### 3）代码中访问hvie

步骤1: 拷贝 hive-site.xml 到 resources 目录下

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://127.0.0.1/hive?createDatabaseIfNotExist=true</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>root</value>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://192.168.101.10:9000</value>
	</property>


</configuration>
```

步骤2: 添加依赖

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-hive_2.11</artifactId>
    <version>2.1.1</version>
</dependency>
```

步骤3：代码

1. 在开发工具中创建数据库默认是在本地仓库.

2. 通过参数修改数据库仓库的地址: **config("spark.sql.warehouse.dir", "hdfs://node1:9000/user/hive/warehouse")**

```scala
package com.vison.sparksql

import org.apache.spark.sql.SparkSession

object HiveReadAndWrite {

  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME","root") //模拟远程用户hadoop用户，不然没有权限执行操作
    //1.创建sparkSession
    val spark = SparkSession.builder()
      .appName("HiveRead")
      .master("local[*]")
      .enableHiveSupport()
      //配置仓库地址，否则就创建在本地了
      .config("spark.sql.warehouse.dir","hdfs://192.168.101.10:9000/usr/hive/warehouse")
      .getOrCreate()

    import  spark.implicits._
    val sc = spark.sparkContext

    //本地的hive，spark内置的hive
    spark.sql("show databases").show
//不建议代码中创建数据库，应该是在hive中创建
    spark.sql("use mydb")
    spark.sql("show tables").show
    spark.sql("select * from person")
	
    

 	//写
     //1.0
    spark.sql("create table user(id int,name string)").show()
    spark.sql("insert into user values(1,'vison')").show()
      //2.0
    val df = spark.read.json("c:/user.json")
      //使用列名匹配存储
    df.write.mode(SaveMode.Append).saveAsTable("person")
      //3.0
      //按照位置一一对应，类似下标对应
	df.write.insertInto("person"); //基本等价于上面的写
      
    spark.close()
  }

}
```



#### 4） 聚合分区数处理

```scala
package com.vison.sparksql

import org.apache.spark.sql.{SaveMode, SparkSession}

object HiveWrite2 {

  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME", "root") //模拟远程用户hadoop用户，不然没有权限执行操作
    //1.创建sparkSession
    val spark = SparkSession.builder()
      .appName("HiveRead")
      .master("local[*]")
      .enableHiveSupport()
      //配置仓库地址，否则就创建在本地了
      .config("spark.sql.warehouse.dir", "hdfs://192.168.101.10:9000/usr/hive/warehouse")
      .getOrCreate()

    import spark.implicits._
    val sc = spark.sparkContext


    val df = spark.read.json("c:/user.json")
    df.createOrReplaceTempView("a")
    val df1 = spark.sql("select * from a")
    val df2 = spark.sql("select sum(age) sumAge from a group by name");

    // 聚合
    println(df1.rdd.getNumPartitions) //1
    println(df2.rdd.getNumPartitions)  //分区数过多(默认200)，直接存储会造成hdfs的文件数量很多 使用coalesce处理  

    df1.write.saveAsTable("a1")
    df2.coalesce(1).write.mode("overwrite").saveAsTable("a2")


    //使用列名匹配存储
    df.write.mode(SaveMode.Append).saveAsTable("person")


    spark.close()
  }

}

```



## 8.实战



```scala
package com.vison.project

import org.apache.spark.sql.SparkSession

/**
 *
 *
 * 需求：各区域热门商品 Top3
 *
 * 计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。
 * 例如:
 * 地区	商品名称		点击次数	城市备注
 * 华北	商品A		100000	北京21.2%，天津13.2%，其他65.6%
 * 华北	商品P		80200	北京63.0%，太原10%，其他27.0%
 * 华北	商品M		40000	北京63.0%，太原10%，其他27.0%
 * 东北	商品J		92000	大连28%，辽宁17.0%，其他 55.0%
 *
 *1.先查询需要的字段
 *  select
 *    ci.*,
 *    pi.product_name,
 *    uva.click_product_id
 *  from user_visit_action uva
 *  join  product_info pi on uva.click_product_id = pi.product_id
 *  join city_info ci on uva.city_id =ci.city_id
 *
 *
 * 2.按照地区和商品名称聚合
 * select
 *   area,
 *   product_name,
 *   count(*) count
 * from t1
 * group by area,product_name
 *
 *
 * 3.按照地区进行分组开窗
 * select
 *    area,
 *    product_name,
 *    count,
 *    rand() over(partition by area order by count desc) rk
 * from t2
 *
 *
 * 4.过滤出来名次小于等于3的
 * select
 *    area
 *    product_name,
 *    count
 *    from t3
 * where rk<=3
 *
 *
 * ------------------------------------
 *
 * sql:
 * CREATE TABLE `user_visit_action`(
 * `date` string,
 * `user_id` bigint,
 * `session_id` string,
 * `page_id` bigint,
 * `action_time` string,
 * `search_keyword` string,
 * `click_category_id` bigint,
 * `click_product_id` bigint,
 * `order_category_ids` string,
 * `order_product_ids` string,
 * `pay_category_ids` string,
 * `pay_product_ids` string,
 * `city_id` bigint)
 * row format delimited fields terminated by '\t';
 * load data local inpath '/opt/module/datas/user_visit_action.txt' into table sparkpractice.user_visit_action;
 *
 * CREATE TABLE `product_info`(
 * `product_id` bigint,
 * `product_name` string,
 * `extend_info` string)
 * row format delimited fields terminated by '\t';
 * load data local inpath '/opt/module/datas/product_info.txt' into table sparkpractice.product_info;
 *
 * CREATE TABLE `city_info`(
 * `city_id` bigint,
 * `city_name` string,
 * `area` string)
 * row format delimited fields terminated by '\t';
 * load data local inpath '/opt/module/datas/city_info.txt' into table sparkpractice.city_info;
 *
 */
object SqlApp {

  def main(args: Array[String]): Unit = {

    System.setProperty("HADOOP_USER_NAME", "root") //模拟远程用户hadoop用户，不然没有权限执行操作
    //1.创建sparkSession
    val spark = SparkSession.builder()
      .appName("HiveRead")
      .master("local[*]")
      .enableHiveSupport()
      //配置仓库地址，否则就创建在本地了
      .config("spark.sql.warehouse.dir", "hdfs://192.168.101.10:9000/usr/hive/warehouse")
      .getOrCreate()
    import spark.implicits._
    val sc = spark.sparkContext

    spark.udf.register("remark",new CityRemarkUDAF)

    spark.sql("use mydb")
    spark.sql("show tables").show
     spark.sql(
      """select
        |  ci.*,
        |  pi.product_name,
        |  uva.click_product_id
        |from user_visit_action uva
        |inner join  product_info pi on uva.click_product_id = pi.product_id
        |inner join city_info ci on uva.city_id =ci.city_id""".stripMargin)
      .createOrReplaceTempView("t1")
    spark.sql("select * from t1").show(10,false)

    spark.sql("""select
                |  area,
                |  product_name,
                |  count(*) count,
                |  remark(city_name) remark
                |from t1
                |group by area,product_name""".stripMargin)
      .createOrReplaceTempView("t2")
    //spark.sql("select * from t2").show()

    spark.sql("""
                |select
                |   area,
                |   product_name,
                |   count,
                |   remark,
                |   rank() over(partition by area order by count desc) rk
                |from t2""".stripMargin)
      .createOrReplaceTempView("t3")

   // spark.sql("select * from t3").show()

    spark.sql("""
                |select
                |   area,
                |   product_name,
                |   remark,
                |   count,rk
                |   from t3
                |where rk<=3""".stripMargin).show(100,false)

    spark.close()

  }

}

```

```scala
package com.vison.project

import java.text.DecimalFormat

import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, LongType, MapType, StringType, StructField, StructType}

class CityRemarkUDAF extends UserDefinedAggregateFunction {

  //入参，城市名
  override def inputSchema: StructType = StructType(Array(StructField("city", StringType)))

  //北京->100，天津->111,成都->101010； 总数
  override def bufferSchema: StructType = StructType(Array(StructField("map", MapType(StringType, LongType)),
    StructField("total", LongType)))

  //北京21.2%，天津13.2%，其他65.6%
  override def dataType: DataType = StringType

  override def deterministic: Boolean = true

  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = Map[String, Long]()
    buffer(1) = 0L
  }

  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    input match {
      case Row(cityName: String) =>
        //1.总数+1
        buffer(1) = buffer.getLong(1) + 1
        //2.城市+1
        val map = buffer.getMap[String, Long](0)
        buffer(0) = map + (cityName -> (map.getOrElse(cityName, 0L) + 1L))
      case _ =>
    }
  }

  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(1) = buffer2.getLong(1) + buffer1.getLong(1)

    val map1 = buffer1.getMap[String, Long](0)
    val map2 = buffer2.getMap[String, Long](0)

    //对于map集合合并
    buffer1(0) = map1.foldLeft(map2) {
      case (map, (city, count)) =>
        map + (city -> (map.getOrElse(city, 0L) + count))
    }

  }

  //北京21.2%，天津13.2%，其他65.6%
  override def evaluate(buffer: Row): Any = {
    val map1 = buffer.getMap[String, Long](0)
    val total = buffer.getLong(1)

    val top2 = map1.toList.sortBy(-_._2).take(2)
    var cityRemarks = top2.map {
      case (cityName, count) => CityRemark(cityName, count.toDouble / total)
    }
    //println(cityRemarks.mkString(","))
    cityRemarks :+= CityRemark("其他", 1-cityRemarks.foldLeft(0D)(_ + _.cityRadio))

    cityRemarks.mkString(",")
  }
}

case class CityRemark(cityName: String, cityRadio: Double) {

  val f = new DecimalFormat("0.00%")

  override def toString: String = s"$cityName:${f.format(cityRadio)}"
}

```

