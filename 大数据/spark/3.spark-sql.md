# 一、Spark SQL 概念

## 1. 什么是 Spark SQL

​		Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块.与基本的 Spark RDD API 不同, Spark SQL 的抽象数据类型为 Spark 提供了关于数据结构和正在执行的计算的更多信息.在内部, Spark SQL 使用这些额外的信息去做一些额外的优化.

​		有多种方式与 Spark SQL 进行交互, 比如: SQL 和 Dataset API. 当计算结果的时候, 使用的是相同的执行引擎, 不依赖你正在使用哪种 API 或者语言.这种统一也就意味着开发者可以很容易在不同的 API 之间进行切换, 这些 API 提供了最自然的方式来表达给定的转换.

​		和 Hive相比，它是将 Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写 MapReduc 的程序的复杂性，由于 MapReduce 这种计算模型执行效率比较慢, 所以 Spark SQL 的应运而生，它是将 Spark SQL 转换成 RDD，然后提交到集群执行，执行效率非常快！

Spark SQL 它提供了2个编程抽象, 类似 Spark Core 中的 **RDD**

- 1. **DataFrame**

- 2.  **DataSet**



## 2. Spark SQL 特点

- **Integrated(易整合)**：无缝的整合了 SQL 查询和 Spark 编程.

- **Uniform Data Access(统一的数据访问方式)：**使用相同的方式连接不同的数据源.

- **Hive Integration(集成 Hive)**：在已有的仓库上直接运行 SQL 或者 HiveQL

- **Standard Connectivity(标准的连接方式)**：通过 JDBC 或者 ODBC 来连接



## 3. **DataFrame**

​		与 RDD 类似，**DataFrame** 也是一个分布式数据容器。然而**DataFrame**更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即**schema**。

​		同时，与**Hive**类似，**DataFrame**也支持嵌套数据类型（**struct**、**array**和**map**）。从 API 易用性的角度上看，**DataFrame API**提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。

![](https://i.bmp.ovh/imgs/2021/04/529c9772c2df32c3.png)





上图直观地体现了**DataFrame**和**RDD**的区别。

​		左侧的**RDD[Person]**虽然以**Person**为类型参数，但Spark框架本身不了解**Person**类的内部结构。而右侧的**DataFrame**却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。

**DataFrame**是为数据提供了**Schema**的视图。可以把它当做数据库中的一张表来对待，**DataFrame**也是懒执行的

性能上比 **RDD**要高，主要原因： 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。





## 4. DataSet

1. 是**DataFrame API**的一个扩展，是 SparkSQL 最新的数据抽象(1.6新增)。

2. 用户友好的API风格，既具有类型安全检查也具有**DataFrame**的查询优化特性。

3.  **Dataset**支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。

4. 样例类被用来在**DataSet**中定义数据的结构信息，样例类中每个属性的名称直接映射到**DataSet**中的字段名称。

5. **DataFrame**是**DataSet**的特列，**DataFrame=DataSet[Row]** ，所以可以通过**as**方法将**DataFrame**转换为**DataSet**。**Row**是一个类型，跟**Car、Person**这些的类型一样，所有的表结构信息都用**Row**来表示。

6. **DataSet**是强类型的。比如可以有**DataSet[Car]**，**DataSet[Person]**.

7. **DataFrame**只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个**String**进行减法操作，在执行的时候才报错，而**DataSet**不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟**JSON**对象和类对象之间的类比。



# 二、Spark SQL 编程

## 1.SparkSession

​			在老的版本中，SparkSQL 提供两种 SQL 查询起始点：一个叫**SQLContext**，用于Spark 自己提供的 SQL 查询；一个叫 **HiveContext**，用于连接 Hive 的查询。

​			从2.0开始, **SparkSession**是 Spark 最新的 SQL 查询起始点，实质上是**SQLContext**和**HiveContext**的组合，所以在**SQLContext**和**HiveContext**上可用的 API 在**SparkSession**上同样是可以使用的。

**SparkSession**内部封装了**SparkContext**，所以计算实际上是由**SparkContext**完成的。当我们使用 spark-shell 的时候, spark 会自动的创建一个叫做**spark**的**SparkSession**, 就像我们以前可以自动获取到一个**sc**来表示**SparkContext**





## 2. 使用DataFrame编程

Spark SQL 的 **DataFrame** API 允许我们使用 **DataFrame** 而不用必须去注册临时表或者生成 SQL 表达式.

**DataFrame** API 既有 **transformation**操作也有**action**操作. **DataFrame**的转换从本质上来说更具有关系, 而 **DataSet** API 提供了更加函数式的 API

ds 功能相比df功能更加强大，不过我们常使用的是df(DataFrame)

### 2.1.创建DataFrame

- 1.1 通过 Spark 的数据源创建：jdbc,json,parquet,hive,scala集合

- 1.2 通过已知的 **RDD** 来创建

- 1.3 通过查询一个 Hive 表来创建.



例子：

```shell
scala> val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df show
warning: there was one feature warning; re-run with -feature for details
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+


scala> 
```



### 2.2.DataFrame的 shell操作

- sql风格
- dsl风格

#### 2.2.1 sql 风格

创建临时表

```bash
df.createOrReplaceTempView (创建或者替换原来的表)

df.createTempView  （创建表）

df.createGlobalTempView (创建全局表，上面的创建只是当前session，这里是全局临时视图表)
```





```shell
scala> val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df show
warning: there was one feature warning; re-run with -feature for details
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+

scala> df.createTempView("emp")

scala> val df1 = spark.sql("select * from emp")
df1: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df1.show
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+

```



#### 2.2.2 dsl 风格

**DataFrame**提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据. 可以在 Scala, Java, Python 和 R 中使用 DSL

使用 DSL 语法风格不必去创建临时视图了.



使用select $ 等来实现，不过常用的还是sql方式

```bash
scala>  val df =spark.read.json("examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]

scala> df.printSchema
root
 |-- name: string (nullable = true)
 |-- salary: long (nullable = true)


scala> df.select($"name",$"salary").show
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+


scala> df.select($"name",$"salary"+100).show
+-------+--------------+
|   name|(salary + 100)|
+-------+--------------+
|Michael|          3100|
|   Andy|          4600|
| Justin|          3600|
|  Berta|          4100|
+-------+--------------+


scala> df.select("name").show
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
|  Berta|
+-------+

scala> df.filter($"salary">4000).show
+----+------+
|name|salary|
+----+------+
|Andy|  4500|
+----+------+

```



## 3 .DF 的代码操作

第一步添加spark-sql的依赖

```xml
<dependency>
     <groupId>org.apache.spark</groupId>
     <artifactId>spark-sql_2.11</artifactId>
     <version>2.1.1</version>
</dependency>
```

#### 3.1 创建DF

```scala
object CreateDf {

  def main(args: Array[String]): Unit = {

    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()

    //2. 通过sparkSession创建DF
    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")

    //3.对DF做操作（sql）
    df.createOrReplaceTempView("user")
    sparkSession.sql("select * from user").show()

    //4.创建sparkSession
    sparkSession.stop()


  }

}
```

#### 3.2 RDD 转DF

```scala
object RDD2DF {

  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()
    val sc = sparkSession.sparkContext
	// 这里需要引入隐式转换
    import sparkSession.implicits._
	//1.通过元组来转换
    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22)::Nil)
    rdd.toDF("name","age").show

	// 2.样例类定义了表结构: 样例类参数名通过反射被读到, 然后成为列名.
    val rdd1 = sc.parallelize(Array(User("hh", 22), User("ss", 11)))
    rdd1.toDF("name","age").show

    sparkSession.stop

  }

}

case class User(name:String,age:Int)
```



#### 3.3 DF 转RDD

```scala
object DF2RDD {

  def main(args: Array[String]): Unit = {

    //1.创建sparkSession
    val spark = SparkSession.builder()
      .appName("createDf")
      .master("local[2]")
      .getOrCreate()

    import spark.implicits._

      //确定是toDF无法确定转换后的数据类型
    val df = (1 to 5).toDF("number")
    df.show()

    // rdd 转为 ,返回的是Row类型
    val rdd: RDD[Row] = df.rdd
    rdd.collect().map(row => row.getInt(0)).foreach(println)

    spark.stop()

  }

}
```



通过api来实现，这里可以确定数据的类型

```scala
object RDD2DF2 {

  def main(args: Array[String]): Unit = {


    //1.创建sparkSession
    val sparkSession = SparkSession.builder().appName("createDf").master("local[2]")
      .getOrCreate()
    val sc = sparkSession.sparkContext

    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22) :: Nil)
      .map {
        case (name, age) => Row(name, age)
      }

    val schema = StructType(Array(StructField("name", StringType),
      StructField("age", IntegerType)))
    val df = sparkSession.createDataFrame(rdd, schema)
    df.show


    sparkSession.stop

  }


}
```



## 4.使用DataSet 进行编程

**DataSet** 和 **RDD** 类似, 但是**DataSet**没有使用 Java 序列化或者 Kryo序列化, 而是使用一种专门的编码器去序列化对象, 然后在网络上处理或者传输.

虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许Spark执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。

**DataSet**是具有**强类型的数据集合**，需要提供对应的类型信息。



```scala
package com.vison.sparksql

import org.apache.spark.sql.{Dataset, SparkSession}

object CreateDS {

  def main(args: Array[String]): Unit = {
    val sparkSession = SparkSession.builder()
      .appName("createDf")
      .master("local[2]")
      .getOrCreate()
    import  sparkSession.implicits._
    val sc = sparkSession.sparkContext
    val list1 = List(10,203,30,20,12,111,2333,22)
    //df能用的ds一定能用的，df只是ds的一种特殊方式

    //1.通过scala的序列得到
     val ds:Dataset[Int] = list1.toDS
    ds.createTempView("tem")
    sparkSession.sql("select * from tem").show


    //2.rdd ds 相互转换得到
    val rdd = sc.parallelize(("ll", 10) :: ("zz", 22)::Nil)
    val ds1 = rdd.toDS()
    ds1.show()

    ds1.rdd.collect().foreach(println) //ds转rdd

    //3.通过ds 和 df想换转换

    //df -> ds
    val df = sparkSession.read.json("D:\\IT\\learning\\scala\\spark\\dsy 08\\users.txt")
    val ds2:Dataset[Employee] = df.as[Employee]
    ds2.show

    //df -> ds
    val df2 = ds2.toDF()
    df2.show
    sparkSession.stop
  }

}
case class Employee(name:String,salary:Long)
```



## 5. RDD,DF,DS 的联系

三者间的相互转换

![](https://i.bmp.ovh/imgs/2021/04/41df5b79a78a3703.png)

在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是**DataFrame**和**DataSet**。他们和RDD有什么区别呢？首先从版本的产生上来看：

RDD (Spark1.0) —> Dataframe(Spark1.3) —> Dataset(Spark1.6)

如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。

在后期的 Spark 版本中，**DataSet**会逐步取代**RDD**和**DataFrame**成为唯一的 API 接口。

### 5.1 共性

- **RDD**、**DataFrame**、**Dataset**全都是 Spark 平台下的分布式弹性数据集，为处理超大型数据提供便利

- 三者都有惰性机制，在进行创建、转换，如**map**方法时，不会立即执行，只有在遇到**Action**如**foreach**时，三者才会开始遍历运算。

- 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出

- 三者都有**partition**的概念

- 三者有许多共同的函数，如**map, filter**，排序等

- 在对 **DataFrame**和**Dataset**进行操作许多操作都需要这个包进行支持 **import spark.implicits._**

- **DataFrame**和**Dataset**均可使用模式匹配获取各个字段的值和类型

### 5.2 区别

##### 5.2.1 **RDD**

*1.* **RDD**一般和**spark mlib**同时使用

*2.* **RDD**不支持**sparksql**操作

##### *5.2.2* **DataFrame**

1. 与**RDD**和**Dataset**不同，**DataFrame**每一行的类型固定为**Row**，每一列的值没法直接访问，只有通过解析才能获取各个字段的值，

2. **DataFrame**与**DataSet**一般不与 spark mlib 同时使用

3. **DataFrame**与**DataSet**均支持 SparkSQL 的操作，比如**select，groupby**之类，还能注册临时表/视窗，进行 sql 语句操作

4. **DataFrame**与**DataSet**支持一些特别方便的保存方式，比如保存成**csv**，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)

##### *5.2.3* **DataSet**

1. **Dataset**和**DataFrame**拥有完全相同的成员函数，区别只是每一行的数据类型不同。 **DataFrame**其实就是**DataSet**的一个特例

2. **DataFrame**也可以叫**Dataset[Row]**,每一行的类型是**Row**，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的**getAS**方法或者共性中的第七条提到的模式匹配拿出特定字段。而**Dataset**中，每一行是什么类型是不一定的，在自定义了**case class**之后可以很自由的获得每一行的信息















