# 1.RDD概念

## 1.1 什么是 RDD

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。

在代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。

## 1.2 RDD 的 5 个主要属性(property)

• **A list of partitions**

  多个分区. 分区可以看成是数据集的基本组成单位.

  对于 RDD 来说, 每个分区都会被一个计算任务处理, 并决定了并行计算的粒度.

  用户可以在创建 RDD 时指定 RDD 的分区数, 如果没有指定, 那么就会采用默认值. 默认值就是程序所分配到的 CPU Core 的数目.

  每个分配的存储是由BlockManager 实现的. 每个分区都会被逻辑映射成 BlockManager 的一个 Block, 而这个 Block 会被一个 Task 负责计算.

• **A function for computing each split**

  计算每个切片(分区)的函数.

  Spark 中 RDD 的计算是以分片为单位的, 每个 RDD 都会实现 **compute** 函数以达到这个目的.

**• A list of dependencies on other RDDs**

  与其他 RDD 之间的依赖关系

  RDD 的每次转换都会生成一个新的 RDD, 所以 RDD 之间会形成类似于流水线一样的前后依赖关系. 在部分分区数据丢失时, Spark 可以通过这个依赖关系重新计算丢失的分区数据, 而不是对 RDD 的所有分区进行重新计算.

• **Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)**

​	对存储键值对的 RDD, 还有一个可选的分区器.

  只有对于 **key-value**的 RDD, 才会有 **Partitioner**, 非**key-value**的 RDD 的 **Partitioner** 的值是 **None**. **Partitiner** 不但决定了 RDD 的本区数量, 也决定了 parent RDD Shuffle 输出时的分区数量.

**• Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)**

存储每个切片优先(preferred location)位置的列表. 比如对于一个 HDFS 文件来说, 这个列表保存的就是每个 **Partition** 所在文件块的位置. 按照“移动数据不如移动计算”的理念, Spark 在进行任务调度的时候, 会尽可能地将计算任务分配到其所要处理数据块的存储位置.



## 1.3 理解 RDD

一个 RDD 可以简单的理解为一个分布式的元素集合.

RDD 表示只读的分区的数据集，对 RDD 进行改动，只能通过 RDD 的转换操作, 然后得到新的 RDD, 并不会对原 RDD 有任何的影响

在 Spark 中, 所有的工作要么是创建 RDD, 要么是转换已经存在 RDD 成为新的 RDD, 要么在 RDD 上去执行一些操作来得到一些计算结果.

每个 RDD 被切分成多个分区(**partition**), 每个分区可能会在集群中不同的节点上进行计算.



##### **1.3.1** ***弹性***

• 存储的弹性：内存与磁盘的自动切换；

• 容错的弹性：数据丢失可以自动恢复；

• 计算的弹性：计算出错重试机制；

• 分片的弹性：可根据需要重新分片。

##### ***1.3.2 分区***

RDD 逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个**compute**函数得到每个分区的数据。

如果 RDD 是通过已有的文件系统构建，则**compute**函数是读取指定文件系统中的数据，如果 RDD 是通过其他 RDD 转换而来，则 **compute**函数是执行转换逻辑将其他 RDD 的数据进行转换。

##### **1.3.3** ***只读***

RDD 是只读的，要想改变 RDD 中的数据，只能在现有 RDD 基础上创建新的 RDD。

由一个 RDD 转换到另一个 RDD，可以通过丰富的转换算子实现，不再像 MapReduce 那样只能写**map**和**reduce**了。

RDD 的操作算子包括两类，

• 一类叫做**transformation**，它是用来将 RDD 进行转化，构建 RDD 的血缘关系；

• 另一类叫做**action**，它是用来触发 RDD 进行计算，得到 RDD 的相关计算结果或者 保存 RDD 数据到文件系统中.

##### **1.3.4** ***依赖(血缘)***

RDDs 通过操作算子进行转换，转换得到的新 RDD 包含了从其他 RDDs 衍生所必需的信息，RDDs 之间维护着这种血缘关系，也称之为依赖。

如下图所示，依赖包括两种，

• 一种是窄依赖，RDDs 之间分区是一一对应的，

• 另一种是宽依赖，下游 RDD 的每个分区与上游 RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。

![](https://i.bmp.ovh/imgs/2020/12/5e115d558662ccb7.png) 

##### **1.3.5** ***缓存***

如果在应用程序中多次使用同一个 RDD，可以将该 RDD 缓存起来，该 RDD 只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该 RDD 的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。

如下图所示，RDD-1 经过一系列的转换后得到 RDD-n 并保存到 hdfs，RDD-1 在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的 RDD-1 转换到 RDD-m 这一过程中，就不会计算其之前的 RDD-0 了。

![](https://i.bmp.ovh/imgs/2020/12/daf224224ccb0024.png) 

##### **1.3.1.6** ***checkpoint***

虽然 RDD 的血缘关系天然地可以实现容错，当 RDD 的某个分区数据计算失败或丢失，可以通过血缘关系重建。

但是对于长时间迭代型应用来说，随着迭代的进行，RDDs 之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。

为此，RDD 支持**checkpoint** 将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint 后的 RDD 不需要知道它的父 RDDs 了，它可以从 checkpoint 处拿到数据。



# 2.RDD编程

## 2.1 RDD 编程模型

在 Spark 中，RDD 被表示为对象，通过对象上的方法调用来对 RDD 进行转换。

经过一系列的**transformations**定义 RDD 之后，就可以调用 actions 触发 RDD 的计算

**action**可以是向应用程序返回结果(**count**, **collect**等)，或者是向存储系统保存数据(**saveAsTextFile**等)。

在Spark中，只有遇到**action**，才会执行 RDD 的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。

要使用 Spark，开发者需要编写一个 Driver 程序，它被提交到集群以调度运行 Worker

Driver 中定义了一个或多个 RDD，并调用 RDD 上的 action，Worker 则执行 RDD 分区计算任务。



## 2.2 RDD 的创建

在 Spark 中创建 RDD 的方式可以分为 3 种：

​	• 从集合中创建 RDD

​	• 从外部存储(本地文件系统, HDFS, Cassandra, HVase, Amazon S3等)创建 RDD

​	• 从其他 RDD 转换得到新的 RDD。



### 1）从集合中创建RDD

```scala
object FromList {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
  
    val arr1 = Array(20,390,39,58.93)
    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)
    // 3. 使用sc创建RDD并执行相应的transformation和action
    val rdd = sc.parallelize(arr1)  //使用parallelize函数创建
      
    val value = sc.makeRDD(arr1) //使用makeRDD函数创建
    rdd.foreach(println)

    sc.stop()
  }

}
```

说明:

• 一旦 RDD 创建成功, 就可以通过并行的方式去操作这个分布式的数据集了.

*•* **parallelize**和**makeRDD**还有一个重要的参数就是把数据集切分成的分区数.

• Spark 会为每个分区运行一个任务(task). 正常情况下, Spark 会自动的根据你的集群来设置分区数



### 2) 从外部存储创建 RDD

Spark 也可以从任意 Hadoop 支持的存储数据源来创建分布式数据集.

可以是本地文件系统, HDFS, Cassandra, HVase, Amazon S3 等等.

Spark 支持 文本文件, SequenceFiles, 和其他所有的 Hadoop InputFormat.

```scala
var distFile = sc.textFile("words.txt")
distFile: org.apache.spark.rdd.RDD[String] = words.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> distFile.collect
res0: Array[String] = Array(atguigu hello, hello world, how are you, abc efg)
```

- *1* **url**可以是本地文件系统文件, **hdfs://...**, **s3n://...**等等

- 2 如果是使用的本地文件系统的路径, 则必须每个节点都要存在这个路径

- 3 所有基于文件的方法, 都支持目录, 压缩文件, 和通配符(*****). 例如: 

`textFile("/my/directory"), textFile("/my/directory/.txt"), and textFile("/my/directory/.gz").`

- 4 **textFile**还可以有第二个参数, 表示分区数. 默认情况下, 每个块对应一个分区.(对 HDFS 来说, 块大小默认是 **128M**). 可以传递一个大于块数的分区数, 但是不能传递一个比块数小的分区数.

### 3) 从其他 RDD 转换得到新的 RDD

就是通过 RDD 的各种转换算子来得到新的 RDD.





## 2.3 RDD的转换transformation

在 RDD 上支持 2 种操作:

- **transformation**：从一个已知的 RDD 中创建出来一个新的 RDD 例如: **map**就是一个**transformation**.

- **action**：在数据集上计算结束之后, 给驱动程序返回一个值. 例如: **reduce**就是一个**action**.



在 Spark 中几乎所有的**transformation**操作都是懒执行的(**lazy**), 也就是说**transformation**操作并不会立即计算他们的结果, 而是记住了这个操作.只有当通过一个**action**来获取结果返回给驱动程序的时候这些转换操作才开始计算.这种设计可以使 Spark 运行起来更加的高效.

默认情况下, 你每次在一个 RDD 上运行一个**action**的时候, 前面的每个**transformed RDD** 都会被重新计算.

但是我们可以通过**persist (or cache)**方法来持久化一个 RDD 在内存中, 也可以持久化到磁盘上, 来加快访问速度. 后面看这种持久化技术.

根据 **RDD** 中数据类型的不同, 整体分为 2 种 **RDD**:

- *1.* **Value**类型

- *2.* **Key-Value**类型(其实就是存一个二维的元组)





### 2.3.1 Value类型

##### *2.3.1.1* **map(func)**

作用: 返回一个新的 RDD, 该 RDD 是由原 RDD 的每个元素经过函数转换后的值而组成. 就是对 RDD 中的数据做转换.

例如：

​	创建一个包含**1-10**的的 RDD，然后将**每个元素\*2**形成新的 RDD



```scala
object FromList1 {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)
    // 得到一个新的 RDD, 但是这个 RDD 中的元素并不是立即计算出来的
    val m1 = sc.parallelize(1 to 10).map(_ * 2)
    // 开始计算 rdd2 中的元素, 并把计算后的结果传递给驱动程序
    val ints = m1.collect()
   ints.foreach(println)
  }
}
```

##### 2.3.1.2 **mapPartitions(func)**

作用: 类似于**map(func)**, 但是是独立在每个分区上运行.所以:**Iterator<T> => Iterator<U>**

假设有**N**个元素，有**M**个分区，那么**map**的函数的将被调用**N**次,而**mapPartitions**被调用**M**次,一个函数一次处理所有分区。



```scala
object MapPartitionsDemo1 {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)
    // 得到一个新的 RDD, 但是这个 RDD 中的元素并不是立即计算出来的
    val m1 = sc.parallelize(1 to 10,2)  // 设置2个分区

    val mp1 = m1.mapPartitions(it => {
      println("sss")
      it.toList //这里会把分区的数据全部加载到内存，可能分区oom，所以一边这里不会使用it.toList
      it.map(_ * 2)
    }).collect()

  }
}
```



##### 2.3.1.3 map()和mapPartitions()的区别

**map()**：每次处理一条数据。

**mapPartitions()**：每次处理一个分区的数据，这个分区的数据处理完后，原 RDD 中该分区的数据才能释放，可能导致 OOM。

开发指导：当内存空间较大的时候建议使用**mapPartitions()**，以提高处理效率。



##### 2.3.1.4 mapPartitionsWithIndex(func)

作用: 和**mapPartitions(func)**类似. 但是会给**func**多提供一个**Int**值来表示分区的索引. 所以**func**的类型是:**(Int, Iterator<T>) => Iterator<U>

```scala
object MapPartitionsWithIndexDemo1 {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)
    // 得到一个新的 RDD, 但是这个 RDD 中的元素并不是立即计算出来的
    val m1:RDD[Int] = sc.parallelize(1 to 10,2)  // 设置2个分区

    val mp1 = m1.mapPartitionsWithIndex((index, it) =>
      it.map(x => (index,x)))

    mp1.foreach(println)

  }
}
```

##### 2.3.1.5 分区数确定原理

```scala
// 1.确定分区数:
override def defaultParallelism(): Int =
   scheduler.conf.getInt("spark.default.parallelism", totalCores)
   
   
//2.对元素进行分区   
// length: RDD 中数据的长度  numSlices: 分区数
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
 (0 until numSlices).iterator.map { i =>
   val start = ((i * length) / numSlices).toInt
   val end = (((i + 1) * length) / numSlices).toInt
   (start, end)
 }
}
seq match {
 case r: Range =>
   
 case nr: NumericRange[_] =>
   
 case _ =>
   val array = seq.toArray // To prevent O(n^2) operations for List etc
   positions(array.length, numSlices).map { case (start, end) =>
       array.slice(start, end).toSeq
   }.toSeq
}   
```



##### 2.3.1.6  **flatMap(func)**

作用: 类似于**map**，但是每一个输入元素可以被映射为 0 或多个输出元素（所以**func**应该返回一个序列，而不是单一元素 **T => TraversableOnce[U]**）

```scala
object FlatMapDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)

      
      //创建一个元素为 1-5 的RDD，运用 flatMap创建一个新的 RDD，新的 RDD 为原 RDD 每个元素的 平方和三次方 来组成 1,1,4,8,9,27..
    val list1 = List(1, 3, 5, 7, 9)

    val fm = sc.parallelize(list1).flatMap(x => Array(x, x * x, x * x * x))
    fm.foreach(println)

    sc.stop()
  }
}
```



##### 2.3.1.7 filter

```scala
object FilterDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)

    val list1 = List(1, 3, 4, 8, 5, 7, 9)

    val fm = sc.parallelize(list1)
      .filter(x => x > 4)
    fm.foreach(println)

    sc.stop()
  }

}
```



##### 2.3.1.8 glom

作用: 将每一个分区的元素合并成一个数组，形成新的 RDD 类型是**RDD[Array[T]]**

```scala
//创建一个 4 个分区的 RDD，并将每个分区的数据放到一个数组
object GlomDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)

    val list1 = List(1, 3, 4, 8, 5, 7, 9)

    val fm = sc.parallelize(list1)
      .glom().map(_.toList)

    //一个分区一个数组，这里会打印4个分组
    fm.foreach(println)
    sc.stop()
  }

}
```



##### 2.3.1.9 **groupBy(func)**

作用:

按照**func**的返回值进行分组.

**func**返回值作为 **key**, 对应的值放入一个迭代器中. 返回的 RDD: **RDD[(K, Iterable[T])**

每组内元素的顺序不能保证, 并且甚至每次调用得到的顺序也有可能不同.

```scala
object GroupByDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    // 2. 创建SparkContext对象
    val sc = new SparkContext(conf)

    //相同的数字放到一起
    val list1 = List(1, 3, 4,4,6,7, 8, 5, 7, 9)

    val fm = sc.parallelize(list1)
      .groupBy(x => x)
    //一个分区一个数组，这里会打印4个分组
    fm.foreach(println)
    sc.stop()
  }

}
```





##### 2.3.1.10 sample(withReplacement, fraction, seed)

作用:

- 以指定的随机种子随机抽样出比例为**fraction**的数据，(抽取到的数量是: **size \* fraction**). 需要注意的是得到的结果并不能保证准确的比例.

- **withReplacement**表示是抽出的数据是否放回，**true**为有放回的抽样，**false**为无放回的抽样. 放回表示数据有可能会被重复抽取到, **false** 则不可能重复抽取到. 如果是**false**, 则**fraction**必须是:**[0,1]**, 是 **true** 则大于等于0就可以了.

- **seed**用于指定随机数生成器种子。 一般用默认的, 或者传入当前的时间戳

```scala
object SampleDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    val list1 = List(1, 3, 4,4,6,7, 8, 5, 7, 9)

    val fm = sc.parallelize(list1)
                .sample(true,0.3)

    fm.foreach(println)
    sc.stop()
  }

}
```



##### 2.3.1.11 distinct([numTasks]))

作用:

对 RDD 中元素执行去重操作. 参数表示任务的数量.默认值和分区数保持一致.

```scala
object SampleDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    val list1 = List(1, 3, 4,4,6,7, 8, 5, 7, 9)

    val fm = sc.parallelize(list1)
                .distinct

    fm.foreach(println)
    sc.stop()
  }

}
```



##### 2.3.1.12 coalesce(numPartitions)

作用: 缩减分区数到指定的数量，用于大数据集过滤后，提高小数据集的执行效率。

减少分区一般不要shuffle

```scala

object CoalesceDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)


    val fm = sc.parallelize(0 to 100)
    println(fm.getNumPartitions) //4

    //减少分区 默认是没有shuffle的，如果一个RDD传递给多个RDD就属于shuffle了
    val fm1 = fm.coalesce(1)
    println(fm1.getNumPartitions) //1
      
    //下面支持了shuffle，这样才能增加分区
    //val fm1 = fm.coalesce(6,true)
    

    sc.stop()
  }

}
```



##### 2.3.1.13 repartition(numPartitions) 

作用: 根据新的分区数, 重新 shuffle 所有的数据, 这个操作总会通过网络.

新的分区数相比以前可以多, 也可以少,底层也是用的coalesce实现

```scala

object RepartitionDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)


    val fm = sc.parallelize(0 to 100)
    println(fm.getNumPartitions) //4

    val fm1 = fm.repartition(6)

    println(fm1.getNumPartitions) //6

    sc.stop()
  }

}
```



##### 2.3.1.14  coalasce 和 repartition的区别

- **coalesce**重新分区，可以选择是否进行**shuffle**过程。由参数**shuffle: Boolean = false/true**决定。

- **repartition**实际上是调用的**coalesce**，进行**shuffle**。源码如下：

```scala
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }

```

- 如果是减少分区, 尽量避免 shuffle



##### 2.3.1.15  sortBy(func,[ascending], [numTasks])

作用: 使用**func**先对数据进行处理，按照处理后结果排序，默认为正序。

```scala
object SortedByDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    val list1 = List(29,39,12,33,42,123,522,32,3444,22)
    //降序
    val fm = sc.parallelize(list1).sortBy(x=>x,false)

    fm.collect().foreach(println)

    sc.stop()
  }

}
```



```scala

case class Person(age:Int,name:String)

object SortedByDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    implicit  val ord:Ordering[Person] = new Ordering[Person] {
      override def compare(x: Person, y: Person): Int = x.age-y.age
    }
    val fm = sc.parallelize(Array(Person(10,"lisi"),Person(20,"zhangshan"),Person(11,"zhangshan")))
      .sortBy(x=>x) // 这里的classTag可以不需要传

    fm.collect().foreach(println)

    sc.stop()
  }

}
```



##### 2.3.1.16  **pipe(command, [envVars])**

作用: 管道，针对每个分区，把 RDD 中的每个数据通过管道传递给**shell**命令或脚本，返回输出的RDD。一个分区执行一次这个命令. 如果只有一个分区, 则执行一次命令.

注意:脚本要放在 worker 节点可以访问到的位置

###### **pipe.sh**

文件内容如下:

```bash
echo "hello"
while read line;do
    echo ">>>"$line
done
```

**创建只有 1 个分区的RDD**

```shell
scala> val rdd1 = sc.parallelize(Array(10,20,30,40), 1)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
#pipe line一个分区执行一次
scala> rdd1.pipe("./pipe.sh").collect
res1: Array[String] = Array(hello, >>>10, >>>20, >>>30, >>>40)
```

**创建有 2 个分区的 RDD**

```shell
scala> val rdd1 = sc.parallelize(Array(10,20,30,40), 2)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:24

#两个分区，执行了两次，hello输出两个
scala> rdd1.pipe("./pipe.sh").collect
res2: Array[String] = Array(hello, >>>10, >>>20, hello, >>>30, >>>40)
```



### 2.3.2 双Value

这里的“双 **Value** 类型交互”是指的两个 **RDD[V]** 进行交互. 并集，交集，差集等

```scala

object ValuesDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list1 = List(1, 3, 5, 6, 7, 3)
    val list2 = List(6, 4, 2, 5, 6, 1)
    val fm1 = sc.parallelize(list1, 2);
    val fm2 = sc.parallelize(list2, 2);

    //1.并集
    val u1 = fm1.union(fm2)
    u1.collect().foreach(println)

    //2.交集 结果不会出现重复元素
    val u2 = fm1.intersection(fm2)
    u2.collect().foreach(println)

    //3.差集,有重复的也是全部去掉
    val u3 = fm1.subtract(fm2)
    u3.collect().foreach(println)


    //4.笛卡尔积,两两配对，n*m个数 尽量避免使用这个
    val u4 = fm1.cartesian(fm2)
    u4.collect().foreach(println)


    //5.拉链操作
    //5.1 这里zip 和scala中不同
    //需要注意的是, 在 Spark 中, 两个 RDD 的元素的数量和分区数都必须相同, 否则会抛出异常.(在 scala 中, 两个集合的长度可以不同)
    val u5 = fm1.zip(fm2)
    u5.collect().foreach(println)


    val list3 = List(1, 3, 5, 6, 7, 3)
    val list4 = List(6, 4, 2, 5)
    val fm3 = sc.parallelize(list3, 2);
    val fm4 = sc.parallelize(list4, 2);

    println("-------------")
    //5.1 zipPartitions拉链操作只要分区相同就可以了 ,it1.zip(it2) 使用的就是scala的拉法
    val u6 = fm3.zipPartitions(fm4)((it1, it2) => {it1.zip(it2)})
    u6.collect().foreach(println)

    println("-------------")
    //5.2 zipWithIndex 自己和自己索引做zip
    val u7 = fm3.zipWithIndex()
    u7.collect().foreach(println)

    sc.stop()
  }

}
```



### 2.3.3 key-value类型

大多数的 Spark 操作可以用在任意类型的 RDD 上, 但是有一些比较特殊的操作只能用在**key-value**类型的 RDD 上.

这些特殊操作大多都涉及到 shuffle 操作, 比如: 按照 key 分组(group), 聚集(aggregate)等.

在 Spark 中, 这些操作在包含对偶类型(**Tuple2**)的 RDD 上自动可用(通过隐式转换).键值对的操作是定义在**PairRDDFunctions**类上, 这个类是对**RDD[(K, V)]**的装饰

```scala
object RDD {
  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])
    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = {
    new PairRDDFunctions(rdd)
  }
```



#### 1）**partitionBy**

作用: 对 pairRDD 进行分区操作，如果原有的 partionRDD 的分区器和传入的分区器相同, 则返回原 pairRDD，否则会生成 ShuffleRDD，即会产生 **shuffle** 过程。



k-v分区是根据key来做分区的，比如HashPartitioner

```scala

object KvDemos {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list2 = List(6, 4, 2, 5, 6, 1)
    val fm2 = sc.parallelize(list2).map((_,1))
    val fm3 = fm2.partitionBy(new HashPartitioner(2))

    //打印每一个分区的数据
    fm3.glom().collect().map(_.toList).foreach(println)


  }

}
```



```scala
object KvDemos1 {

    // 按照value来做分区，可以通过下面先变为v-k 分区后在变回来实现
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list2 = List(6, 4, 2, 5, 6, 1)
    val fm2 = sc.parallelize(list2).map((_,1))
    val fm3 = fm2.map({
      case (k, v) => (v, k)
    }).partitionBy(new HashPartitioner(2)) // hashcode取模实现，可能不太理想，某些区太多数据了
      .map({
        case (k, v) => (v, k)
      })
    fm3.glom().collect().map(_.toList).foreach(println)
    // 结果，第一个分区没有数据
    //List()
    //List((6,1), (4,1), (2,1), (5,1), (6,1), (1,1))
  }

}
```



#### 2）reduceByKey

作用: 在一个**(K,V)**的 RDD 上调用，返回一个**(K,V)**的 RDD，使用指定的**reduce**函数，将相同**key**的**value**聚合到一起，**reduce**任务的个数可以通过第二个可选的参数来设置。

分区内聚合和分区间聚合逻辑相同

```scala
object ReduceByKeyDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list2 = List("hello", "world","hello", "me", "this", "ok")
    val fm2 = sc.parallelize(list2).map((_,1))

    //统计每一个单词出现次数，这里可以传入分区数，默认是这个rdd之前所有经历分区的最大数
    fm2.reduceByKey(_+_).foreach(println)
  }

}
```



#### 3）groupByKey

作用: 按照**key**进行分组.

```scala
object GroupByKeyDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list2 = List("hello", "world","hello", "me", "this", "ok")
    val fm2 = sc.parallelize(list2).map((_,1))

    //只能按照key分组
    fm2.groupByKey().collect().foreach(println)
  }




  /**
   *  reduceByKey和GroupByKey区别
   * 1.一个做聚合，一个做分组
   * 2.性能差别：groupByKey 做聚合传递的数量量相比reduceByKey较大，IO消耗大，性能相比要差一些
   */
}
```



-  **groupByKey**必须在内存中持有所有的键值对. 如果一个**key**有太多的**value**, 则会导致内存溢出(OutOfMemoryError)

- 所以groupByKey这操作非常耗资源, 如果分组的目的是为了在每个**key**上执行聚合操作(比如: **sum** 和 **average**), 则应该使用**PairRDDFunctions.aggregateByKey** 或者**PairRDDFunctions.reduceByKey**, 因为他们有更好的性能(会先在分区进行预聚合)

#### 4）foldByKey

折叠，也是聚合，有预聚合功能，”零“值只有在分区内聚合使用，分区间聚合不使用

```scala

object FoldByKeyDemo1 {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val list2 = List("hello", "world","hello", "me", "this", "ok","hello","hello")
    val fm2 = sc.parallelize(list2).map((_,1))

    val fm3 = fm2.foldByKey(1,5)(_ + _)
    fm3.collect().foreach(println)

    /**
     * (this,2)
     * (me,2)
     * (hello,6)
     * (world,2)
     * (ok,2)
     */
    // zeroValue的只在分区内聚合有效,每一个分区有数据的时候会操作

  }
}
```



#### 5）**aggregateByKey**

1.reduceByKey,foldByKey都是有预聚合的，分区内聚合逻辑和分区间的聚合逻辑都是一样的

2.aggregateBykey实现了分区聚合和分区间聚合的逻辑可以不一样

```scala
object aggregateByKeyDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val fm1 = sc.parallelize(List(("a",3),("a",2),("c",4),("b",3),("c",6),("c",8)),2)


    /**
     * 1. 如下操作是 在各个分区内获取最大值，然后在相加
     *参数说明：

     */
    // val re1 = fm1.aggregateByKey(Int.MinValue)((u,v)=>u.max(v), (u1,u2)=>u1+u2)
    val re = fm1.aggregateByKey(Int.MinValue)(_.max(_), _ + _) //简写
    re.collect().foreach(println)
      
      
      
   /**
     * 2. 如下操作是 在各个分区内获取最大值和最小值，然后在相加
     */
    //val re2 = fm1.aggregateByKey((Int.MinValue,Int.MaxValue))((u,v)=>(u._1.max(v),u._2.min(v)), (u1,u2)=>(u1._1+u2._1,u2._2+u1._2))

    //通过 偏函数 简写上面的
    val re3 = fm1.aggregateByKey((Int.MinValue,Int.MaxValue))({
      case ((max,min),v) => (max.max(v),min.min(v))
    },{
      case ((max1,min1),(max2,min2)) => (max1+max2,min1+min2)
    })

    re3.collect().foreach(println)
    //(b,(3,3))
    //(a,(3,2))
    //(c,(12,10))   
      
    /**
     * 3.通过aggregateByKey计算平均值
     */
    val re4 = fm1.aggregateByKey((0, 0))({
      case ((sum, count), v) => (sum + v, count + 1)
    }, {
      case ((sum1, count1), (sum2, count2)) => (sum1 + sum2, count1 + count2)
    }).map({
      case (key,(sum,count)) => (key,sum.doubleValue()/count)
    })
    re4.collect().foreach(println)
    //(b,3.0)
    //(a,2.5)
    //(c,6.0)   
  }

}

	/*
     * zeroValue => “零”值,分区内使用
     * seqOp => 分区内操作
     * combOp => 分区间操作
     */
//解释
  def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,
      combOp: (U, U) => U): RDD[(K, U)] = self.withScope {
    aggregateByKey(zeroValue, defaultPartitioner(self))(seqOp, combOp)
  }

// (seqOp: (U, V) => U  :这个类似fold，第一个u最开始是zeroValue，v表示key的value值，通过操作计算出下一次操作的u值，进行第二次计算，再次给有相同的可以使用作为当前u，和下一个相同key的value做操作
// combOp: (U, U) => U  : U表示不同分区的值做聚合操作
```





#### 6）combineByKey

​		分区内聚合和分区间绝活逻辑不同，相比aggregateBykey的"零值"不是写死的，而是碰到的每一个key的第一个value来生成的,所以更加灵活和复杂

```scala
  def combineByKey[C](
      createCombiner: V => C,
      mergeValue: (C, V) => C,
      mergeCombiners: (C, C) => C): RDD[(K, C)] = self.withScope {
    combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null)
  }

//先解释
    /**
     * createCombiner: V => C : 可以认为是创建“零值”的,v表示key第一次出现的value，c表示生成的值
     * mergeValue: (C, V) => C ： 分区内的聚合,调用这个函数进行合并操作. 分区内合并
     * mergeCombiners: (C, C) => C) ： 分区间的聚合,跨分区合并相同的key的值(C). 跨分区合并
     * 
     */

```

例子

```scala
object combineByKeyDemo1 {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(("a", 88), ("b", 95), ("a", 91), ("b", 93), ("a", 95), ("b", 98)), 2)

    //求每一个key和
    val res = cb.combineByKey(v => v,(c1:Int,v1) => v1+c1,(c2:Int,c3:Int) =>c2+c3)
    res.collect().foreach(println)

  }
}
```





#### 7）四个聚合参数的关联

combineByKey，reduceBykey,aggregateByKey,combineByKey

底层都是调用如下函数

```scala
combineByKeyWithClassTag[V]((v: V) => v, func, func, partitioner)


  def combineByKeyWithClassTag[C](
      createCombiner: V => C,
      mergeValue: (C, V) => C,
      mergeCombiners: (C, C) => C,
      partitioner: Partitioner,
      mapSideCombine: Boolean = true,
      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope {
      
      ....
  }
```



#### 8）sortByKey

作用: 在一个**(K,V)**的 RDD 上调用, **K**必须实现 **Ordered[K]** 接口(或者有一个隐式值: **Ordering[K]**), 返回一个按照**key**进行排序的**(K,V)**的 RDD

排序的时候尽量使用spark的排序，避免使用scala排序，否则很容易出现oom



```scala
object sortByKeyDemo1 {


  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(("a", 88), ("b", 95), ("a", 91), ("b", 93), ("a", 95), ("b", 98)), 2)

    //根据key排序
    val res = cb.sortByKey(false,2)
    res.collect().foreach(println)

  }
}
```



#### 9) cogoup

作用：在类型为**(K,V)**和**(K,W)**的 RDD 上调用，返回一个**(K,(Iterable<V>,Iterable<W>))**类型的 RDD

```scala
object coGroupDemo1 {


  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(("a", 88), ("b", 95), ("a", 91), ("b", 93), ("a", 95), ("b", 98)), 2)
    val cb1 = sc.parallelize(Array(("a", 3), ("b", 5), ("a", 23), ("b", 22), ("a", 95), ("b", 11)), 2)

    //两个 rdd分区组合成一个集合，放到元组中
    val res = cb.cogroup(cb1)
    res.collect().foreach(println)

    //(b,(CompactBuffer(95, 93, 98),CompactBuffer(5, 22, 11)))
    //(a,(CompactBuffer(88, 91, 95),CompactBuffer(3, 23, 95)))

  }
}
```





#### 10. **join(otherDataset, [numTasks])**

内连接,和sql类似

在类型为**(K,V)**和**(K,W)**的 RDD 上调用，返回一个相同 key 对应的所有元素对在一起的**(K,(V,W))**的RDD



### 2.3.4 案例

统计每一个省份广告被点击的次数top3

```scala
/**
     * 时间戳 省份 城市 用户 广告
     *  12313121231312 6 7 63 16
     	12313121231312 7 7 24 16
     	12313121231312 8 7 64 11
     	....
     *
     * 1.统计每一个省份广告被点击的次数top3
     * 猜想：倒推法
     *    RDD[(省份，List((广告1,19),(广告1,3),(广告1,3))...)] 排序取前三 groupByKey
     *    <= RDD[((省份，广告)，10)...] reduceByKey
     *    <= RDD[((省份，广告)，1)...] map
     */

    val rdd = sc.textFile("d:/ll.log")
    rdd.map(line => {
            val split = line.split(" ")
            ((split(1), split(4)), 1)
          })
      .reduceByKey(_ + _)
      .map({ case ((pro, ads), count) => (pro, (ads, count)) })
      .groupByKey()
      .map({
            case (pro,adsCountIt) => (pro,adsCountIt.toList.sortBy(_._2)(Ordering.Int.reverse).take(3))  //排序取前三
         })
      .collect()
      .foreach(println)

  }
```



## 2.4 action行动算子

### 1）collect

​		以数组的形式返回 RDD 中的所有元素.所有的数据都会被拉到 driver 端, 所以要慎用

### 2）count

​		返回 RDD 中元素的个数.

### 3）take

​	返回 RDD 中前 n 个元素组成的数组.take 的数据也会拉到 driver 端, 应该只对小数据集使用

### 4）first

​		返回 RDD 中的第一个元素. 类似于**take(1)**,但是返回的是一个元素不是take返回的数组

### 5）takeOrdered(n, [ordering])

​		返回排序后的前 **n** 个元素, 默认是升序排列.  数据也会拉到 driver 端

### 6）countByKey

​	作用：针对**(K,V)**类型的 RDD，返回一个**(K,Int)**的**map**，表示每一个**key**对应的元素个数。



### 7）reduce

​	通过**func**函数聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据。

```scala
object activeDemo1 {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(1,34,54,65,34))
    cb.filter(x=> {
      println(x)  //打印出数据表示执行了
      x >0
    })
    .reduce(_+_)

  }
}
```



### 8）aggregate

```scala
object activeDemo1 {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(1,34,54,65,34))
    val re = cb.filter(x => {
      println(x) //打印出数据表示执行了
      x > 0
    })
      .aggregate(0)(_+_,_+_)

    println(re)

  }
}
```



### 9）fold

​		折叠操作，**aggregate**的简化操作，**seqop**和**combop**一样的时候,可以使用**fold**

```scala
object activeDemo1 {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(1,34,54,65,34))
    val re = cb.filter(x => {
      println(x) //打印出数据表示执行了
      x > 0
    })
      .fold(1)(_ + _)

    println(re)

  }
}
```

### 10 聚合算子和转换聚合算子的不同处

fold和aggregate行动算子和转换聚合算子的零值参与计算次数不同。

- 聚合算子在分区内聚合会计算一次，在分区间聚合还会计算一次，转换聚合算子只会在分区内聚合计算一次

### 

### 11）foreach

​	作用: 针对 RDD 中的每个元素都执行一次**func**

​	每个函数是在 Executor 上执行的, 不是在 driver 端执行的.和scala完全不同

```scala
object activeDemo1 {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(1,34,54,65,34))
     cb.filter(x => {
      x > 0
    }).foreach(println) //每次打印可能不同，因为不同分区实在不同的executor中实现的，
    
    
  }
}
```



### 12 foreachPartition

对于每一个分区的元素做统一处理

```scala
object activeDemo1 {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array(1,34,54,65,34))
     cb.filter(x => {
      x > 0
    }).foreachPartition(it => it.foreach(println)) //it 指各自分区的数据

  }
}
```



### 13）saveAsTextFile

​		作用：将数据集的元素以**textfile**的形式保存到**HDFS**文件系统或者其他支持的文件系统，对于每个元素，Spark 将会调用**toString**方法，将它装换为文件中的文本

### 14）**saveAsSequenceFile**

​		作用：将数据集中的元素以 Hadoop sequencefile 的格式保存到指定的目录下，可以使 HDFS 或者其他 Hadoop 支持的文件系统。

### 15）**saveAsObjectFile**

​	作用：用于将 RDD 中的元素序列化成对象，存储到文件中。



## 2.5 函数传递（序列化）

​			我们进行 Spark 进行编程的时候, 初始化工作是在 **driver**端完成的, 而实际的运行程序是在**executor**端进行的. 所以就涉及到了进程间的通讯, 数据是需要序列化的.

- driver端制定计划（DAG），分配任务
- executor做具体的执行动作，所有传递给算子的那些函数都是在executor执行的。如果函数中包含了对象的属性那么是需要实现的序列化的



### 1） Serializable

spark默认使用这个序列化

- 1.函数默认提供了序列化
- 2 让类实现序列化接口:**Serializable**
- 3 传递局部变量而不是属性.

```scala

object SeralDemo1 {


  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array("hello33","workd","helel","hello"))

	//过滤包含hello的字符
    val searcher = new Searcher("hello")
     searcher.getMatchedRDD1(cb).foreach(println)
     searcher.getMatchedRDD2(cb).foreach(println)
     searcher.getMatchedRDD3(cb).foreach(println)

  }
}

class Searcher(val query:String)  extends Serializable{

  def isMatch(s:String) ={
    s.contains(query)
  }


  def getMatchedRDD1(rdd:RDD[String]) ={

    //filter 在driver上执行，isMatch在executor上执行，name这个方法可能实在不同的进程执行，所以这个对象是需要实现序列化
    rdd.filter(isMatch)
  }

  def getMatchedRDD2(rdd:RDD[String]) ={
    // query是对象属性，所以类也是需要实现序列化
    rdd.filter(x => x.contains(query))
  }

  def getMatchedRDD3(rdd:RDD[String]) ={
    val q = query
    //这里的q是局部变量不是对象的属性，所以这里可以不需要类实现序列化
    rdd.filter(x => x.contains(q))
  }
}
```



### 2) Kyro序列化

参考地址: https://github.com/EsotericSoftware/kryo

Java 的序列化比较重, 能够序列化任何的类. 比较灵活,但是相当的慢, 并且序列化后对象的体积也比较大.

Spark 出于性能的考虑, 支持另外一种序列化机制: kryo (2.0开始支持). kryo 比较快和简洁.(速度是**Serializable**的10倍). 想获取更好的性能应该使用 kryo 来序列化.

从2.0开始, Spark 内部已经在使用 kryo 序列化机制: 当 RDD 在 **Shuffle**数据的时候, 简单数据类型, 简单数据类型的数组和字符串类型已经在使用 kryo 来序列化.

有一点需要注意的是: **即使使用 kryo 序列化, 也要实现 Serializable 接口.**

```scala
# 1.注册需要序列化的类
val conf: SparkConf = new SparkConf()
      //.set("spark.serializer",classOf[Kryo].getName) //更新序列化器
      .registerKryoClasses(Array(classOf[Searcher])) //注册需要使用kryo序列化的类，这里默认设置Kyro序列化器
      .setMaster("local[2]").setAppName("WordCount")
# 2.需要序列化的类同样需要实现 Serializable 接口
```



```scala

object KryoDemo1 {


  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf()
      .set("spark.serializer",classOf[Kryo].getName) //更新序列化器
      .registerKryoClasses(Array(classOf[Searcher])) //注册需要使用kryo序列化的类
      .setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val cb = sc.parallelize(Array("hello","workd","helel","hello"))


    val searcher = new Searcher("hello")
     searcher.getMatchedRDD1(cb).foreach(println)
     searcher.getMatchedRDD2(cb).foreach(println)
     searcher.getMatchedRDD3(cb).foreach(println)

  }
}

class Searcher(val query:String) extends Serializable {

  def isMatch(s:String) ={
    s.contains(query)
  }


  def getMatchedRDD1(rdd:RDD[String]) ={

    //filter 在driver上执行，isMatch在executor上执行，name这个方法可能实在不同的进程执行，所以这个对象是需要实现序列化
    rdd.filter(isMatch)
  }

  def getMatchedRDD2(rdd:RDD[String]) ={
    // query是对象属性，所以类也是需要实现序列化
    rdd.filter(x => x.contains(query))
  }

  def getMatchedRDD3(rdd:RDD[String]) ={
    val q = query
    //这里的q是局部变量不是对象的属性，所以这里可以不需要类实现序列化
    rdd.filter(x => x.contains(q))
  }
}
```



## 2.6 RDD 的依赖关系

### 1） 查看RDD的血缘关系

```scala
object RddRelation {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    val fm = sc.parallelize(Array(("helo",1),("thanks",2),("yes",3),("match",1)))
      .map({case (k,v) => (k,v+v)})
      .filter(x => x._1.contains("he"))
      .groupByKey() //有shuffle，分区

    println(fm.toDebugString) //查看血缘关系,从下往上看，看左边对其看是否在一个分区上执行2表示分区
/*(2) ShuffledRDD[3] at groupByKey at RddRelation.scala:18 []
   +-(2) MapPartitionsRDD[2] at filter at RddRelation.scala:17 []
      |  MapPartitionsRDD[1] at map at RddRelation.scala:16 []
      |  ParallelCollectionRDD[0] at parallelize at RddRelation.scala:15 []
*/    
    sc.stop()

  }

}
```



### 2） 查看RDD的依赖关系



```scala
object RddRelation {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    val fm = sc.parallelize(Array(("helo",1),("thanks",2),("yes",3),("match",1)))
    val fm1=fm .map({case (k,v) => (k,v+v)})
    val fm2=fm1.filter(x => x._1.contains("he"))
    val fm3=fm2.groupByKey() //有shuffle，分区

    println(fm.dependencies)  //List() 第一个没有依赖
    println(fm1.dependencies) //List(org.apache.spark.ShuffleDependency@57bd2029)
    println(fm2.dependencies) //List(org.apache.spark.ShuffleDependency@57bd2029)
    println(fm3.dependencies) //List(org.apache.spark.ShuffleDependency@57bd2029)

    sc.stop()

  }

}
```

​		RDD 之间的关系可以从两个维度来理解: 一个是 RDD 是从哪些 RDD 转换而来, 也就是 RDD 的 parent RDD(s)是什么; 另一个就是 RDD 依赖于 parent RDD(s)的哪些 Partition(s). 这种关系就是 RDD 之间的依赖.

#### 2.1）窄依赖

如果 B RDD 是由 A RDD 计算得到的, 则 B RDD 就是 Child RDD, A RDD 就是 parent RDD.

如果依赖关系在设计的时候就可以确定, 而不需要考虑父 RDD 分区中的记录, 并且如果父 RDD 中的每个分区最多只有一个子分区, 这样的依赖就叫窄依赖

一句话总结: 父 RDD 的每个分区最多被一个 RDD 的分区使用

![](https://i.bmp.ovh/imgs/2020/12/68cab74763ed3820.png)

具体来说, 窄依赖的时候, 子 RDD 中的分区要么只依赖一个父 RDD 中的一个分区(比如**map**, **filter**操作), 要么在设计时候就能确定子 RDD 是父 RDD 的一个子集(比如: **coalesce**).

所以, 窄依赖的转换可以在任何的的一个分区上单独执行, 而不需要其他分区的任何信息.



#### 2.2）宽依赖

如果 父 RDD 的分区被不止一个子 RDD 的分区依赖, 就是宽依赖.

![](https://i.bmp.ovh/imgs/2020/12/5ac0403562eec65d.png)

宽依赖工作的时候, 不能随意在某些记录上运行, 而是需要使用特殊的方式(比如按照 key)来获取分区中的所有数据.

例如: 在排序(**sort**)的时候, 数据必须被分区, 同样范围的 **key** 必须在同一个分区内. 具有宽依赖的 **transformations** 包括: **sort**, **reduceByKey**, **groupByKey**, **join**, 和调用**rePartition**函数的任何操作.



## 2.7 Spark Job的划分

由于 Spark 的懒执行, 在驱动程序调用一个**action**之前, Spark 应用不会做任何事情.

针对每个 **action**, Spark 调度器就创建一个执行图(execution graph)和启动一个 **Spark job**

每个 job 由多个**stages** 组成, 这些 **stages** 就是实现最终的 RDD 所需的数据转换的步骤. 一个宽依赖划分一个 stage.

每个 **stage** 由多个 **tasks** 来组成, 这些 **tasks** 就表示每个并行计算, 并且会在多个执行器上执行.

![](https://i.bmp.ovh/imgs/2020/12/b405a8677c60af61.png)

### 1）DAG(Directed Acyclic Graph) 有向无环图

Spark 的顶层调度层使用 RDD 的依赖为每个 job 创建一个由 stages 组成的 DAG(有向无环图). 在 Spark API 中, 这被称作 DAG 调度器(DAG Scheduler).

我们已经注意到, 有些错误, 比如: 连接集群的错误, 配置参数错误, 启动一个 Spark job 的错误, 这些错误必须处理, 并且都表现为 DAG Scheduler 错误. 这是因为一个 Spark job 的执行是被 DAG 来处理.

DAG 为每个 job 构建一个 stages 组成的图表, 从而确定运行每个 task 的位置, 然后传递这些信息给 TaskSheduler. TaskSheduler 负责在集群中运行任务.





### 2） Jobs

Spark job 处于 Spark 执行层级结构中的最高层. 每个 Spark job 对应一个 action, 每个 action 被 Spark 应用中的驱动所程序调用.

可以把 Action 理解成把数据从 RDD 的数据带到其他存储系统的组件(通常是带到驱动程序所在的位置或者写到稳定的存储系统中)

只要一个 action 被调用, Spark 就不会再向这个 job 增加新的东西.



### 3） Stage

前面说过, RDD 的转换是懒执行的, 直到调用一个 action 才开始执行 RDD 的转换.

正如前面所提到的, 一个 job 是由调用一个 action 来定义的. 一个 action 可能会包含一个或多个转换( transformation ), Spark 根据宽依赖把 job 分解成 stage.

从整体来看, 一个 stage 可以任务是“计算(task)”的集合, 这些每个“计算”在各自的 Executor 中进行运算, 而不需要同其他的执行器或者驱动进行网络通讯. 换句话说, 当任何两个 workers 之间开始需要网络通讯的时候, 这时候一个新的 stage 就产生了, 例如: shuffle 的时候.

这些创建 stage 边界的依赖称为 **ShuffleDependencies**. shuffle 是由宽依赖所引起的, 比如: **sort**, **groupBy**, 因为他们需要在分区中重新分发数据. 那些窄依赖的转换会被分到同一个 stage 中.

因为边界 stage 需要同驱动进行通讯, 所以与 job 有关的 stage 通常必须顺序执行而不能并行执行.

如果这个 stage 是用来计算不同的 RDDs, 被用来合并成一个下游的转换(比如: **join**), 也是有可能并行执行的. 但是仅需要计算一个 RDD 的宽依赖转换必须顺序计算.

所以, 设计程序的时候, 尽量少用 **shuffle**.



### 4）Tasks

stage 由 tasks 组成. 在执行层级中, task 是最小的执行单位. 每一个 task 表现为一个本地计算.

一个 stage 中的所有 tasks 会对不同的数据执行相同的代码.(程序代码一样, 只是作用在了不同的数据上)

一个 task 不能被多个执行器来执行, 但是, 每个执行器会动态的分配多个 slots 来执行 tasks, 并且在整个生命周期内会并行的运行多个 task. 每个 stage 的 task 的数量对应着分区的数量, 即每个 Partition 都被分配一个 Task 



例子：

```scala
  def simpleSparkProgram(rdd:RDD[Double]):Long = {
    
    rdd.filter(_<1000.0) //stage1
      .map(x=>(x,x))
      .groupByKey()      //stage2
      .map({case (value,group) =>(group.sum,value)})
      .sortByKey()      //stage3
      .count()
  }
  
```

![](https://i.bmp.ovh/imgs/2020/12/1f8de42b4798271b.png)

在大多数情况下, 每个 stage 的所有 task 在下一个 stage 开启之前必须全部完成



## 2.8 RDD 的持久化

每碰到一个 Action 就会产生一个 job, 每个 job 开始计算的时候总是从这个 job 最开始的 RDD 开始计算.



问题：

- 每调用一次 **collect**, 都会创建一个新的 job, 每个 job 总是从它血缘的起始开始计算. 所以, 会发现中间的这些计算过程都会重复的执行.

- 原因是因为 **rdd**记录了整个计算过程. 如果计算的过程中出现哪个分区的数据损坏或丢失, 则可以从头开始计算来达到容错的目的.

```scala
object CacheDemo {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.parallelize(Array("ab", "bc"))
    val rdd2 = rdd1.flatMap(x => {
      println("flatMap...")
      x.split("")
    })
    val rdd3: RDD[(String, Int)] = rdd2.map(x => {
      (x, 1)
    })

    rdd3.collect.foreach(println)  //rdd3会执行一次job
    println("-----------")
    rdd3.collect.foreach(println) //rdd3会在执行一次job
     //这里的collect都会从头开始计算
  }
}
```



### 1）持久化

每个 job 都会重新进行计算, 在有些情况下是没有必要, 如何解决这个问题呢?

Spark 一个重要能力就是可以持久化数据集在内存中. 当我们持久化一个 RDD 时, 每个节点都会存储他在内存中计算的那些分区, 然后在其他的 action 中可以重用这些数据. 这个特性会让将来的 action 计算起来更快(通常块 10 倍). 对于迭代算法和快速交互式查询来说, 缓存(Caching)是一个关键工具.

可以使用方法**persist()**或者**cache()**来持久化一个 RDD. 在第一个 action 会计算这个 RDD, 然后把结果的存储到他的节点的内存中. Spark 的 Cache 也是容错: 如果 RDD 的任何一个分区的数据丢失了, Spark 会自动的重新计算.

RDD 的各个 Partition 是相对独立的, 因此只需要计算丢失的部分即可, 并不需要重算全部 Partition



```scala
object CacheDemo {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
    val sc = new SparkContext(conf)

    val rdd1 = sc.parallelize(Array("ab", "bc"))
    val rdd2 = rdd1.flatMap(x => {
      println("flatMap...")
      x.split("")
    })
    val rdd3: RDD[(String, Int)] = rdd2.map(x => {
      (x, 1)
    })

    rdd3.persist(StorageLevel.MEMORY_ONLY)
    rdd3.cache()  //等同于 persist() ,等同于 persist(StorageLevel.MEMORY_ONLY)
    rdd3.collect.foreach(println)  //rdd3会执行一次job
    println("-----------")
    rdd3.collect.foreach(println) //上面有缓存了，就不会从头再次执行rdd3
  }
}
```

另外, 允许我们对持久化的 RDD 使用不同的存储级别.

例如: 可以存在磁盘上, 存储在内存中(堆内存中), 跨节点做复本.

```scala
## persist 支持如下几种缓存RDD，见名知义，可以存储在磁盘和内存及副本数
object StorageLevel {
  val NONE = new StorageLevel(false, false, false, false)
  val DISK_ONLY = new StorageLevel(true, false, false, false)
  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
  val MEMORY_ONLY = new StorageLevel(false, true, false, true)
  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)
}
```



### 2）聚合算子默认缓存

有一点需要说明的是, 即使我们不手动设置持久化, Spark 也会自动的对一些 shuffle 操作的中间数据做持久化操作(比如: reduceByKey). 这样做的目的是为了当一个节点 shuffle 失败了避免重新计算整个输入. 当时, 在实际使用的时候, 如果想重用数据, 仍然建议调用**persist** 或 **cache**





## 2.9 检查点

Spark 中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制,检查点（本质是通过将RDD写入Disk做检查点）是为了通过 Lineage 做容错的辅助

Lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销。

检查点通过将数据写入到 HDFS 文件系统实现了 RDD 的检查点功能。

为当前 RDD 设置检查点。该函数将会创建一个二进制的文件，并存储到 checkpoint 目录中，该目录是用 **SparkContext.setCheckpointDir()**设置的。在 checkpoint 的过程中，该RDD 的所有依赖于父 RDD中 的信息将全部被移除。



```scala
object CacheDemo {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")

    val sc = new SparkContext(conf)
    sc.setCheckpointDir("./ch1") //设置checkpoint目录
    val rdd1 = sc.parallelize(Array("ab", "bc"))
    val rdd2 = rdd1.flatMap(x => {
      println("flatMap...")
      x.split("")
    })
    val rdd3: RDD[(String, Int)] = rdd2.map(x => {
      (x, 1)
    })

    rdd3.checkpoint()	//这里表示有一个计划，当后面遇到一个job后，就会模仿它执行一次前面的流程
    rdd3.collect.foreach(println)  //rdd3会执行一次job ，上面的checkpoint模仿它执行之前的一遍流程然后存储计算结果
    println("-----------")
    rdd3.collect.foreach(println) //上面有checkpoint，就不会从头再次执行rdd3，checkpoint中拿数据
  }
}
```

**检查点需要做的事情**

- 设置检查点目录
- 对需要checkpoint的rdd做checkpoint



**注意：**

- checkpoint的时候会重新启动一个job单独做checkpoint

- 会切断作业的血缘关系，后面的就是通过checkpoint做父的血缘关系

- 为了避免checkpoint重复执行一次计算，一般和cache联合使用，checkpoint放前，cache放后

  - ```scala
    //因为checkpoint是以模仿后面第一个job，所以使用缓存，那么就可以直接从缓存中获取数据，不会再去从头执行一遍前面的逻辑
    rdd3.checkpoint() 
    rdd3.cache()
    ```

    

**和持久化比对**

- 需要手动设置存储目录

- 会多启动一个job

- 会切断作业的血缘关系

  

# 3.分区器



对于只存储 **value**的 RDD, 不需要分区器.

只有存储**Key-Value**类型的才会需要分区器.

Spark 目前支持 Hash 分区和 Range 分区，用户也可以自定义分区.

Hash 分区为当前的默认分区，Spark 中分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 过程后属于哪个分区和 Reduce 的个数.、

## 3.1.查看分区器

```scala
object PartitionerDemo {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val rdd1 = sc.parallelize(Array("ab", "bc"))
    val rdd3: RDD[(String, Int)] = rdd1.map(x => {
      (x, 1)
    })

    println(rdd3.partitioner) //None
   val rdd4 = rdd3.map(x => (x._1 + "1", x._2 + 1))
      .partitionBy(new HashPartitioner(3))
    println(rdd4.partitioner) //Some(org.apache.spark.HashPartitioner@3)
  }
}
```





## 3.2.HashPartitioner

**HashPartitioner**分区的原理：对于给定的**key**，计算其**hashCode**，并除以分区的个数取余，如果余数小于 0，则用**余数+分区的个数**（否则加0），最后返回的值就是这个**key**所属的分区**ID**。





## 3.3.RangePartitioner

**HashPartitioner** 分区弊端： 可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有 RDD 的全部数据。比如某些极端, 他们都进入了 0 分区.

**RangePartitioner** 作用：将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。

实现过程为：

- 1）先从整个 RDD 中抽取出样本数据（水塘抽样，每个数据被抽到概率相同）

- 2）将样本数据排序，从样本数据中计算出每个分区的最大 **key**就是每个分区的最大 **key** 值，形成一个**Array[KEY]**类型的数组变量 **rangeBounds**；(边界数组).  边界数组数量的多少是(parition-1)

- 3）判断**key**在**rangeBounds**中所处的范围，给出该**key**值在下一个**RDD**中的分区**id**下标；该分区器要求 RDD 中的 **KEY** 类型必须是可以排序的.



比如[1,100,200,300,400]，表示6个分区，小于1的一个分区，1-100的是一个分区.....，然后对比传进来的**key**，返回对应的分区**id**。



```scala
sortByKey() // 如果分区数大于1，如果分区数据变化了内部会使用RangePartitioner执行collect，所以会执行
//但是小于1不会用范围分区数


---------------------

//源码 查看分析
  // TODO: this currently doesn't work on P other than Tuple2!
  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
      : RDD[(K, V)] = self.withScope
  {
    val part = new RangePartitioner(numPartitions, self, ascending)
    new ShuffledRDD[K, V, V](self, part)
      .setKeyOrdering(if (ascending) ordering else ordering.reverse)
  }



//RangePartitioner
class RangePartitioner[K : Ordering : ClassTag, V](
    partitions: Int,
    rdd: RDD[_ <: Product2[K, V]],
    private var ascending: Boolean = true)
  extends Partitioner {


  // An array of upper bounds for the first (partitions - 1) partitions
  private var rangeBounds: Array[K] = {
		。......
        if (imbalancedPartitions.nonEmpty) {
          // Re-sample imbalanced partitions with the desired sampling probability.
          val imbalanced = new PartitionPruningRDD(rdd.map(_._1), imbalancedPartitions.contains)
          val seed = byteswap32(-rdd.id - 1)
          val reSampled = imbalanced.sample(withReplacement = false, fraction, seed).collect()  //这里有collect行动算子
          val weight = (1.0 / fraction).toFloat
          candidates ++= reSampled.map(x => (x, weight))
        }
        RangePartitioner.determineBounds(candidates, partitions)
      }
    }
  }
      ....
      
  }
```







## 3.4.自定义分区器

要实现自定义的分区器，你需要继承 **org.apache.spark.Partitioner**, 并且需要实现下面的方法:

- **numPartitions**：该方法需要返回分区数, 必须要大于0.

- **getPartition(key)**：  返回指定键的分区编号(0到numPartitions-1)。

- **equals**：  Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同

- **hashCode**： 如果你覆写了**equals**, 则也应该覆写这个方法.



```scala
object PartitionerDemo {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val rdd1 = sc.parallelize(Array("ab", "bc"))

    val rdd3: RDD[(String, Int)] = rdd1.map(x => {
      (x, 1)
    })
      .partitionBy(new MyPartitioner(3))
    println(rdd3.partitioner) //None

  }
}

/**
 * 自定义分区器
 * @param num
 */
class MyPartitioner(num:Int) extends Partitioner{
  //分区数
  override def numPartitions: Int =  {
    assert(num>0)
    num
  }

  //分区
  override def getPartition(key: Any): Int = key match {
    case null => 0
    case _ => key.hashCode().abs % num

  }
}
```



# 4.文件中的数据读取和保存

从文件中读取数据是创建 RDD 的一种方式.

把数据保存的文件中的操作是一种 Action.

Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。

文件格式分为：Text文件、Json文件、csv文件、Sequence文件以及Object文件；

文件系统分为：本地文件系统、HDFS、Hbase 以及 数据库。

平时用的比较多的就是: 从 HDFS 读取和保存 Text 文件.



## 4.1 textFile读取文件



```scala
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    //1. 可以给textFile目录，但是要保证目录中不能再有目录
    // 2. Read a text file from HDFS, a local file system (available on all nodes), or any
    // Hadoop-supported file system URI, and return it as an RDD of Strings.
    var rdd1 = sc.textFile("./word.txt") //这里存储在本地
	rdd1.flatMap(_.split(" ")).map((_, 1))
			.reduceByKey(_ +_)
			.saveAsTextFile("./output") //保存为文本文件本地

```



## 4.2 读取json文件



如果 JSON 文件中每一行就是一个 JSON 记录，那么可以通过将 JSON 文件当做文本文件来读取，然后利用相关的 JSON 库对每一条数据进行 JSON 解析。

注意：使用 RDD 读取 JSON 文件处理很复杂，同时 SparkSQL 集成了很好的处理 JSON 文件的方式，所以实际应用中多是采用SparkSQL处理JSON文件。

```scala
import scala.util.parsing.json.JSON


sc.textFile("./word.txt")
      .map(JSON.parseFull)  //json解析
      .saveAsTextFile("./out")
```



## 4.3 SequenceFile 

只针对PairRDD才有这个方式做出了i

SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对而设计的一种平面文件(Flat File)。

Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用 **sequenceFile[ keyClass, valueClass](path)**。

注意：SequenceFile 文件只针对 PairRDD

```scala
object ReadWriteFileDemo {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    sc.parallelize(Array(("a",1),("b",2),("a",1),("b",2)))
      .saveAsSequenceFile("/output")

    //读取的key是String类型，value是Int类型
    sc.sequenceFile[String,Int]("./input")

  }

}
```



## 4.4 Object文件

对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。

可以通过**objectFile[k,v](path)** 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用**saveAsObjectFile()** 实现对对象文件的输出

```scala

object ReadWriteFileDemo {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    sc.parallelize(Array(("a",1),("b",2),("a",1),("b",2)))
      .saveAsObjectFile("./output")

    //读取的对象是一个元组 String，Int类型
    sc.objectFile[(String,Int)]("./input")

  }

}
```



## 4.5 Hdfs文件

Spark 的整个生态系统与 Hadoop 完全兼容的,所以对于 Hadoop 所支持的文件类型或者数据库类型,Spark 也同样支持.

另外,由于 Hadoop 的 API 有新旧两个版本,所以 Spark 为了能够兼容 Hadoop 所有的版本,也提供了两套创建操作接口.

对于外部存储创建操作而言,HadoopRDD 和 newHadoopRDD 是最为抽象的两个函数接口,主要包含以下四个参数.

1）输入格式(InputFormat): 制定数据输入的类型,如 TextInputFormat 等,新旧两个版本所引用的版本分别是 org.apache.hadoop.mapred.InputFormat 和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)

2）键类型: 指定[K,V]键值对中K的类型

3）值类型: 指定[K,V]键值对中V的类型

4）分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits

注意:其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值。

1. 在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压.

2. 如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成上面的hadoopRDD和newAPIHadoopRDD两个类就行了



```scala
object ReadWriteFileDemo {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    sc.parallelize(Array(("a",1),("b",2),("a",1),("b",2)))
      .saveAsTextFile("hadoop://192.168.101.6:9000/output")

    //读取文件
    sc.textFile("hadoop://192.168.101.6:9000/input")

  }

}
```



## 4.6 Mysql 数据读写文件



```scala

object ReadWriteFileDemo {


  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)

    //1. 读取数据
    new JdbcRDD[String](sc, () => {
      //建立连接
      Class.forName("com.mysql.jdbc.Driver")
      var url = "jdbc:mysql://127.0.0.1:3306";
      DriverManager.getConnection(url, "root", "root")
    }, "select * from books where id > ? and id < ?",
      1, //下限 替换上面的占位符
      100, //上限 替换上面的占位符
      3,
      resultSet => {
        resultSet.getString(2)

        ""
      }

    ).collect().foreach(println)



    //2.写入jdbc
    Class.forName("com.mysql.jdbc.Driver")
    var url = "jdbc:mysql://127.0.0.1:3306";
    val rdd = sc.parallelize(Array((20, "zhanshan"), (20, "lili"), (20, "lisi"), (44, "zhouwei")))
    //写入数据，不过注意每一条数据都foreach，这里连接数会过多，所以这里实际不能这样用，应该使用rdd.foreachPartition()

    rdd.foreach({
      case (age, name) => {
        val connection = DriverManager.getConnection(url, "root", "root")
        val statement = connection.prepareStatement("insert into user values(?,?)")
        statement.setInt(1, age)
        statement.setString(2, name)
        statement.execute()
        statement.close()
        connection.close()
      }
    })


    //分区，一个分区一个连接
    rdd.foreachPartition(it => {
      val connection = DriverManager.getConnection(url, "root", "root")
      it.foreach({
        case (age, name) => {
          val statement = connection.prepareStatement("insert into user values(?,?)")
          statement.setInt(1, age)
          statement.setString(2, name)
          statement.execute()
          statement.close()
          connection.close()
        }
      })

    })
  }

}
```



## 4.7 Hbase读取写入文件

由于 **org.apache.hadoop.hbase.mapreduce.TableInputFormat** 类的实现，Spark 可以通过Hadoop输入格式访问 HBase。

这个输入格式会返回键值对数据，其中键的类型为**org. apache.hadoop.hbase.io.ImmutableBytesWritable**，而值的类型为**org.apache.hadoop.hbase.client.Result**。

```scala
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-server</artifactId>
    <version>1.3.1</version>
    <exclusions>
        <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>servlet-api-2.5</artifactId>
        </exclusion>
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
        </exclusion>
    </exclusions>

</dependency>

<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-client</artifactId>
    <version>1.3.1</version>
    <exclusions>
        <exclusion>
            <groupId>org.mortbay.jetty</groupId>
            <artifactId>servlet-api-2.5</artifactId>
        </exclusion>
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
        </exclusion>
    </exclusions>
</dependency>
```



#### 1) 读取数据

```scala


object HbaseDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val hbaseConfig = HBaseConfiguration.create()
    hbaseConfig.set("habase.zookeeper.quorum","zk1,zk2,zk3")
    hbaseConfig.set(TableInputFormat.INPUT_TABLE,"studen")
      
    //从hbase读取数据
    val rdd1 = sc.newAPIHadoopRDD(hbaseConfig,
      classOf[TableInputFormat],
      classOf[ImmutableBytesWritable],
      classOf[Result]
    )

    val rdd2 = rdd1.map({
      //iw 只封装rowkey，result封装一行数据(有多列)
      case (iw, result) => {
        val map = mutable.Map[String, Any]()
        map+="rowkey" -> Bytes.toString(iw.get())
        import scala.collection.JavaConversions._
        val cells = result.listCells()

        for (cell <- cells){
          // 列名->列值
          val key = Bytes.toString(CellUtil.cloneQualifier(cell))
          val value = Bytes.toString(CellUtil.cloneValue(cell))
          map += key->value
        }
        //map 将map转换为json字符串
       implicit  val dff = DefaultFormats
        Serialization.write(map)
      }
    })

    rdd2.foreach(println)

  }

}
```



#### 2) 写数据



```scala
object HbaseWriteDemo {

  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val hbaseConfig = HBaseConfiguration.create()
    hbaseConfig.set("habase.zookeeper.quorum", "zk1,zk2,zk3")
    hbaseConfig.set(TableOutputFormat.OUTPUT_TABLE,"studen")


    val job = Job.getInstance(hbaseConfig)
    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])
    job.setOutputKeyClass(classOf[ImmutableBytesWritable])
    job.setOutputValueClass(classOf[Put])

    val rdd = sc.parallelize(Array(("1",20, "zhanshan"),
      ("2",20, "lili"), ("3",20, "lisi"), ("4",44, "zhouwei")))

    //先封装rdd格式为tableReduce的格式
    val hbaseRdd = rdd.map({
      case (rk, age, name) => {
        val rowkey = new ImmutableBytesWritable()
        rowkey.set(Bytes.toBytes(rk))

        val put = new Put(Bytes.toBytes(rk));
        put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("age"), Bytes.toBytes(age))
        put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("name"), Bytes.toBytes(name))
        (rowkey,put)
      }
    })

    hbaseRdd.saveAsNewAPIHadoopDataset(hbaseConfig)

    sc.stop()
  }
}
```

#### 3) 读取文件切片数

可以查看源码，根据文件数，文件大小有关





# 5.累加器

​		累加器用来对信息进行聚合，通常在向 Spark 传递函数时，比如使用 **map()** 函数或者用 **filter()** 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，所以更新这些副本的值不会影响驱动器中的对应变量。

如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。

累加器是一种变量, 仅仅支持“add”, 支持并发. 累加器用于去实现计数器或者求和. Spark 内部已经支持数字类型的累加器, 开发者可以添加其他类型的支持.



**潜规则：**累计器一般只在行动算子中使用，可以保证每个 task 只执行一次. 如果放在 **transformations** 操作中则不能保证只更新一次.有可能会被重复执行.

## 5.1 内置累加器

说明:

1. 在驱动程序中通过**sc.longAccumulator**得到**Long**类型的累加器, 还有**Double**类型的

2. 可以通过**value**来访问累加器的值.(与**sum**等价). **avg**得到平均值

3. 只能通过**add**来添加值.

4. 累加器的更新操作最好放在**action**中, Spark 可以保证每个 task 只执行一次. 如果放在 **transformations** 操作中则不能保证只更新一次.有可能会被重复执行.

```scala

object AccuDemo {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val fm = sc.parallelize(0 to 100)

    var a = 0
    val rd = fm.map(x => {
      a += 1
      x
    })

    rd.collect()
    println(a) // 0


    //1.累加器 累加
    val acc = sc.longAccumulator
    val rd1 = fm.map(x => {
      acc.add(1)
      x
    })

    rd1.collect()
    println(acc.value) // 101
  }

}

```





## 5.2 自定义累加器

在使用自定义累加器的不要忘记注册**sc.register(acc)**

```scala
import org.apache.spark.util.AccumulatorV2
import org.apache.spark.{SparkConf, SparkContext}

object AccuDemo {

  def main(args: Array[String]): Unit = {

    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")
    //  创建SparkContext对象
    val sc = new SparkContext(conf)
    val fm = sc.parallelize(0 to 100)

    //自定义累加器
    val acc1 = new MyIntAcc
    sc.register(acc1,"firt")

    val rd2 = fm.map(x => {
      acc1.add(1)
      x
    })

    rd2.collect()
    println(acc1.value) // 101

  }

}


/**
 *  AccumulatorV2[IN, OUT] IN 表示输入类型，Out表示输出类型
 */
class MyIntAcc extends AccumulatorV2[Int,Int]{
  private var sum = 0

  //判“零” ，对于缓冲区的值进行判“零”
  override def isZero: Boolean = sum == 0

  //复制累加器
  override def copy(): AccumulatorV2[Int, Int] = {

    val acc = new MyIntAcc
    acc.sum= sum
    acc

  }

  //重置累加器,把缓存区值置位‘零’
  override def reset(): Unit = sum ==0

  //真正累加的方法（分区内累加）
  override def add(v: Int): Unit = sum+=v

  //分区间合并
  override def merge(other: AccumulatorV2[Int, Int]): Unit = {
    other match {
      case acc:MyIntAcc => sum+=acc.sum
      case _ => sum+=0
    }
  }

  //返回累加后的最终值
  override def value: Int = sum



}

```





## 5.3 广播变量

广播变量在每个节点上保存**一个只读的变量的**缓存, 而不用给每个 task 来传送一个 copy. 可以有效节省内存占用，每一个节点共同使用一个变量比每一个task节约得多。

例如, 给每个节点一个比较大的输入数据集是一个比较高效的方法. Spark 也会用该对象的广播逻辑去分发广播变量来降低通讯的成本.

广播变量通过调用**SparkContext.broadcast(v)**来创建. 广播变量是对**v**的包装, 通过调用广播变量的 **value**方法可以访问.



```scala

object BroadcastDemo {

  def main(args: Array[String]): Unit = {

    val bigArr = 1 to 10000 toArray
    val conf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("WordCount")

    //  创建SparkContext对象
    val sc = new SparkContext(conf)
      //创建广播变量
    val bd = sc.broadcast(bigArr)

    
    val fm = sc.parallelize(0 to 100)

    //通过value获取值
    val rd = fm.filter(x => bd.value.contains(x))

  }
}
```



