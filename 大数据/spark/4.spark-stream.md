spark-core,spark-sql 都是处理离线式数据，spark-stream就是实时处理数据，准备来说是准实时处理

离线处理时长，但是指标可以处理很多

实时数据的指标一般较少，也更加复杂

# 1.Spark Streaming 概述

Spark Streaming 是 Spark 核心 API 的扩展, 用于构建弹性, 高吞吐量, 容错的在线数据流的流式处理程序. 总之一句话: Spark Streaming 用于流式数据的处理

数据可以来源于多种数据源: Kafka, Flume, Kinesis, 或者 TCP 套接字. 接收到的数据可以使用 Spark 的负责元语来处理, 尤其是那些高阶函数像: **map, reduce, join, 和window**.

最终, 被处理的数据可以发布到 FS（fileSystem）, 数据库或者在线dashboards.

另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合.



在 Spark Streaming 中，处理数据的单位是一批而不是单条，而数据采集却是逐条进行的，因此 Spark Streaming 系统需要设置间隔使得数据汇总到一定的量后再一并操作，这个间隔就是批处理间隔。批处理间隔是 Spark Streaming 的核心概念和关键参数，它决定了 Spark Streaming 提交作业的频率和数据处理的延迟，同时也影响着数据处理的吞吐量和性能。

![](https://i.bmp.ovh/imgs/2021/05/32248c536c14b8ea.png)

Spark Streaming 提供了一个高级抽象: discretized stream(SStream), DStream 表示一个连续的数据流.

DStream 可以由来自数据源的输入数据流来创建, 也可以通过在其他的 DStream 上应用一些高阶操作来得到.

在内部, 一个 DSteam 是由一个个 RDD 序列来表示的.



## 1.1 Spark Streaming 特点

优点：易用，容错，易整合到Spark体系中

缺点：Spark Streaming 是一种“微量批处理”架构, 和其他基于“一次处理一条记录”架构的系统相比, 它的**延迟会相对高**一些.



## 1.2 Spark Streaming 架构

![](https://i.bmp.ovh/imgs/2021/05/f00f3ce6dc28e385.png)

**背压机制**

Spark 1.5以前版本，用户如果要限制 Receiver 的数据接收速率，可以通过设置静态配制参数**spark.streaming.receiver.maxRate**的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：**producer**数据生产高于**maxRate**，当前集群处理能力也高于**maxRate**，这就会造成资源利用率下降等问题。

为了更好的协调数据接收速率与资源处理能力，1.5版本开始 Spark Streaming 可以动态控制数据接收速率来适配集群数据处理能力。背压机制（即Spark Streaming Backpressure）: 根据 JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。

通过属性**spark.streaming.backpressure.enabled**来控制是否启用backpressure机制，默认值**false**，即不启用。



# 2. DStream 入门

## 2.1 wordcount 案例

**需求：**使用 netcat 工具向 9999 端口不断的发送数据，通过 Spark Streaming 读取端口数据并统计不同单词出现的次数；常用来做测试

依赖

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.11</artifactId>
    <version>2.1.1</version>
</dependency>
```



代码

```scala
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

object StreamingWordCount {
    def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[*]")
        // 1. 创建SparkStreaming的入口对象: StreamingContext  参数2: 表示事件间隔   内部会创建 SparkContext
        val ssc = new StreamingContext(conf, Seconds(3))
        // 2. 创建一个DStream
        val lines: ReceiverInputDStream[String] = ssc.socketTextStream("node1", 9999)
        // 3. 一个个的单词
        val words: DStream[String] = lines.flatMap(_.split("""\s+"""))
        // 4. 单词形成元组
        val wordAndOne: DStream[(String, Int)] = words.map((_, 1))
        // 5. 统计单词的个数
        val count: DStream[(String, Int)] = wordAndOne.reduceByKey(_ + _)
        //6. 显示
        count.print
        //7. 开始接受数据并计算
        ssc.start()
        // 7.1 count.print 这个不会使用
        //8. 等待计算结束(要么手动退出,要么出现异常)才退出主程序
        ssc.awaitTermination()
        
        //每次输入数据，都会打印新数据的统计结果例如：（a,3） (b,1) (a,1)
    }
}


//1.node1 上启动 netcat
		nc -lk 9999
//2.可以打包到 linux 启动我们的 wordcount, 也可以在 idea 直接启动.
// 3.查看输出结果. 每 3 秒统计一次数据的输入情况.
```



**需要注意的：**

1. 一旦**StreamingContext**已经启动, 则不能再添加添加新的 streaming computations，即上面的例子中第7步要放在ssc.start之前

2. 一旦一个**StreamingContext**已经停止(**StreamingContext.stop()**), 他也不能再重启

3. 在一个 JVM 内, 同一时间只能启动一个**StreamingContext**

4. **stop()** 的方式停止**StreamingContext**, 也会把**SparkContext**停掉. 如果仅仅想停止**StreamingContext**, 则应该这样: **stop(false)**

5. 一个**SparkContext**可以重用去创建多个**StreamingContext**, 前提是以前的**StreamingContext**已经停掉,并且**SparkContext**没有被停掉





## 2.2 wordcount 分析

Discretized Stream(DStream) 是 Spark Streaming 提供的基本抽象, 表示持续性的数据流, 可以来自输入数据, 也可以是其他的 DStream 转换得到. 在内部, 一个 DSteam 用连续的一系列的 RDD 来表示. 在 DStream 中的每个 RDD 包含一个确定时间段的数据.

![](https://i.bmp.ovh/imgs/2021/05/87b578d42be6f109.png)





对 DStream 的任何操作都会转换成对他里面的 RDD 的操作. 比如前面的 wordcount 案例, **flatMap**是应用在 line DStream 的每个 RDD 上, 然后生成了 words SStream 中的 RDD. 如下图所示:

![](https://i.bmp.ovh/imgs/2021/05/afad045c1920bc22.png)



对这些 RDD 的转换是有 Spark 引擎来计算的. DStream 的操作隐藏的大多数的细节, 然后给开发者提供了方便使用的高级 API.

![](https://i.bmp.ovh/imgs/2021/05/e105ea4450abf36f.png)





## 2.3 Dstream 创建

测试过程中，可以通过使用**ssc.queueStream(queueOfRDDs)**来创建**DStream**，每一个推送到这个队列中的**RDD**，都会作为一个**DStream**处理。常用来做测试



案例：循环创建几个 RDD，将 RDD 放入队列。通过 Spark Streaming创建 Dstream，计算 WordCount



```scala
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.collection.mutable

object RDDQueueDemo {
    def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("RDDQueueDemo").setMaster("local[*]")
        val scc = new StreamingContext(conf, Seconds(5))
        val sc = scc.sparkContext
        
        // 创建一个可变队列
        val queue: mutable.Queue[RDD[Int]] = mutable.Queue[RDD[Int]]()
        //oneAtATime true表示队列一次放一个
        val rddDS: InputDStream[Int] = scc.queueStream(queue, true)
        rddDS.reduce(_ + _).print
        
        scc.start
        
        // 循环的方式向队列中添加 RDD
        for (elem <- 1 to 5) {
            queue += sc.parallelize(1 to 100)
            Thread.sleep(2000)
        }
        
        scc.awaitTermination()
    }
}
```



## 2.4 自定义数据源

其实就是自定义接收器；需要继承**Receiver**，并实现**onStart**、**onStop**方法来自定义数据源采集。

目的：自定义数据源，实现监控某个端口号，获取该端口号内容。



```scala
package com.vison.sparkstream

import java.io.{BufferedReader, InputStreamReader}
import java.net.Socket
import java.nio.charset.StandardCharsets

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.receiver.Receiver

/**
 * 自定义数据源，实现监控某个端口号，获取该端口号内容。
 */
object ReceiverDemo {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[*]")
    // 1. 创建SparkStreaming的入口对象: StreamingContext  参数2: 表示事件间隔
    val ssc = new StreamingContext(conf, Seconds(5))
    // 2. 创建一个DStream
    val lines: ReceiverInputDStream[String] = ssc.receiverStream[String](new MyReceiver("node1", 9999))
    // 3. 一个个的单词
    lines.flatMap(_.split("""\s+"""))
      .map((_,1))
      .reduceByKey((_+_))
      .print
    //4. 显示
    count.print
    //5. 启动流式任务开始计算
    ssc.start()
    //6. 等待计算结束才退出主程序
    ssc.awaitTermination()
    ssc.stop(false)

  }

  /**
   * onStart: 1.启动子线程来接受数据
   * 2.接受到的数据通过调用store(data) 传递给其他执行器处理
   * 3.如果发生异常，重启接收器restart,会自动调用onStop和onStart方法
   *
   * onStop: 1.关闭资源
   */
  class MyReceiver(host: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_ONLY) {
    var socket: Socket = null;
    var reader: BufferedReader = null;

    /**
     *
     * 接收器启动的时候调用该方法. This function must initialize all resources (threads, buffers, etc.) necessary for receiving data.
     * 这个函数内部必须初始化一些读取数据必须的资源
     * 该方法不能阻塞, 所以 读取数据要在一个新的线程中进行.
     *
     */
    override def onStart(): Unit = {
      // 启动一个新的线程来接收数据
      new Thread("Socket Receiver") {
        override def run(): Unit = {
          receive()
        }
      }.start()
    }

    override def onStop(): Unit = {
      // 循环结束, 则关闭资源
      if (reader != null) reader.close()
      if (socket != null) socket.close()
    }

    // 此方法用来接收数据
    def receive() = {
      try {
        socket = new Socket(host, port)
        reader = new BufferedReader(new InputStreamReader(socket.getInputStream, StandardCharsets.UTF_8))
        var line: String = null
        // 当 receiver没有关闭, 且reader读取到了数据则循环发送给spark
        while (!isStopped && (line = reader.readLine()) != null) {
          // 发送给spark
          store(line)
        }
      } catch {
        case e => e.printStackTrace
      } finally {
        // 重启任务
        restart("Trying to connect again")
      }
    }
  }

}

```



# 3. Kafka 数据源

注意kafka和stream的版本结合不同，需要查看官网。

在工程中需要引入 Maven 依赖 **spark-streaming-kafka_2.11**来使用它。

包内提供的 **KafkaUtils** 对象可以在 **StreamingContext**和**JavaStreamingContext**中以你的 Kafka 消息创建出 **DStream**。

两个核心类：**KafkaUtils、KafkaCluster**



## 3.1 streaming-kafka 使用概念

参考：https://spark.apache.org/docs/latest/streaming-programming-guide.html#linking



1.基于Receiver ：

- 一个Receiver负责接收数据，，然后存储到executor中，然后交给sparkStream去处理数据
- 默认有可能有丢失数据，至多一次
- 可以启动WALs(预写日志)，不会丢失数据，至少一次



2.直连kafka相比基于Receiver：

- 并行度：Stream会创建和kafaka的topic相同分区的RDd分区数
- 高效：可以不使用WAL,可以直接基于kafka的存储获取数据
- 严格一次：保证spark-stream去消费kafka消息严格一次，而基于Receiver的WAL可能会出现多次消费



**使用**

依赖：

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
    <version>2.1.1</version>
</dependency>
```



代码：

```scala
package com.vison.sparkstream

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka.KafkaUtils

object KafkaWordCount {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[*]")
    // 1. 创建SparkStreaming的入口对象: StreamingContext  参数2: 表示事件间隔
    val ssc = new StreamingContext(conf, Seconds(5))

    val kafkaParams = Map[String, String](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "192.168.4.157:9092;192.168.4.158:9092",
      ConsumerConfig.GROUP_ID_CONFIG -> "myGroup")

    val sourseStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set("test"))

    sourseStream
      .flatMap {
        case (_, v) => v.split("\\w+")}
      .map((_, 1))
      .reduceByKey(_ + _)
      .print

    ssc.start()
    ssc.awaitTermination()
    ssc.stop(false)
  }
}
```

避免数据丢失使用

```scala
package com.vison.sparkstream

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}

object KafkaWordCount1 {

  def createSS() = {
    val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[*]")
    // 1. 创建SparkStreaming的入口对象: StreamingContext  参数2: 表示事件间隔
    val ssc = new StreamingContext(conf, Seconds(5))
    ssc.checkpoint("chk1") //设置checkpoint目录

    val kafkaParams = Map[String, String](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "192.168.4.157:9092;192.168.4.158:9092",
      ConsumerConfig.GROUP_ID_CONFIG -> "myGroup")

    val sourseStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set("test"))

    sourseStream
      .flatMap {
        case (_, v) => v.split("\\w+")
      }
      .map((_, 1))
      .reduceByKey(_ + _)
      .print

    ssc;
  }

  /**
   * 默认都是从最新的数据开始消费，如果中途重新启动，那么之前kafka新的数据就没有办法消费，
   *  所以stream有一个check-point 来记录之前的记录
   * 从checkpoint中恢复一个streamingContext，如果没有checkpoint那么会新创建一个
   *
   */
  def main(args: Array[String]): Unit = {
    // chk1 从这个目录创建StreamingContext
    val ssc = StreamingContext.getActiveOrCreate("chk1", createSS)

    ssc.start()
    ssc.awaitTermination()
  }
}
```

**使用checkpoint：**

- 数据不会丢失也不会重复
- 缺点：小文件过多
- 企业中很少使用，一般数据的跟踪是开发者自己来完成



```scala
import kafka.common.TopicAndPartition
import kafka.message.MessageAndMetadata
import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka.KafkaCluster.Err
import org.apache.spark.streaming.kafka.{HasOffsetRanges, KafkaCluster, KafkaUtils, OffsetRange}
import org.apache.spark.streaming.{Seconds, StreamingContext}

object LowKafka {
    // 获取 offset
    def getOffset(kafkaCluster: KafkaCluster, group: String, topic: String): Map[TopicAndPartition, Long] = {
        // 最终要返回的 Map
        var topicAndPartition2Long: Map[TopicAndPartition, Long] = Map[TopicAndPartition, Long]()
        
        // 根据指定的主体获取分区信息
        val topicMetadataEither: Either[Err, Set[TopicAndPartition]] = kafkaCluster.getPartitions(Set(topic))
        // 判断分区是否存在
        if (topicMetadataEither.isRight) {
            // 不为空, 则取出分区信息
            val topicAndPartitions: Set[TopicAndPartition] = topicMetadataEither.right.get
            // 获取消费消费数据的进度
            val topicAndPartition2LongEither: Either[Err, Map[TopicAndPartition, Long]] =
                kafkaCluster.getConsumerOffsets(group, topicAndPartitions)
            // 如果没有消费进度, 表示第一次消费
            if (topicAndPartition2LongEither.isLeft) {
                // 遍历每个分区, 都从 0 开始消费
                topicAndPartitions.foreach {
                    topicAndPartition => topicAndPartition2Long = topicAndPartition2Long + (topicAndPartition -> 0)
                }
            } else { // 如果分区有消费进度
                // 取出消费进度
                val current: Map[TopicAndPartition, Long] = topicAndPartition2LongEither.right.get
                topicAndPartition2Long ++= current
            }
        }
        // 返回分区的消费进度
        topicAndPartition2Long
    }
    
    // 保存消费信息
    def saveOffset(kafkaCluster: KafkaCluster, group: String, dStream: InputDStream[String]) = {
        
        dStream.foreachRDD(rdd => {
            var map: Map[TopicAndPartition, Long] = Map[TopicAndPartition, Long]()
            // 把 RDD 转换成HasOffsetRanges对
            val hasOffsetRangs: HasOffsetRanges = rdd.asInstanceOf[HasOffsetRanges]
            // 得到 offsetRangs
            val ranges: Array[OffsetRange] = hasOffsetRangs.offsetRanges
            ranges.foreach(range => {
                // 每个分区的最新的 offset
                map += range.topicAndPartition() -> range.untilOffset
            })
            kafkaCluster.setConsumerOffsets(group,map)
        })
    }
    
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("HighKafka")
        val ssc = new StreamingContext(conf, Seconds(3))
        // kafka 参数
        //kafka参数声明
        val brokers = "node1:9092,node2:9092,node3:9092"
        val topic = "first"
        val group = "bigdata"
        val deserialization = "org.apache.kafka.common.serialization.StringDeserializer"
        val kafkaParams = Map(
            "zookeeper.connect" -> "node1:2181,node2:2181,node3:2181",
            ConsumerConfig.GROUP_ID_CONFIG -> group,
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers,
            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> deserialization,
            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> deserialization
        )
        // 读取 offset
        val kafkaCluster = new KafkaCluster(kafkaParams)
        val fromOffset: Map[TopicAndPartition, Long] = getOffset(kafkaCluster, group, topic)
        val dStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, String](
            ssc,
            kafkaParams,
            fromOffset,
            (message: MessageAndMetadata[String, String]) => message.message()
        )
        dStream.print()
        // 保存 offset
        saveOffset(kafkaCluster, group, dStream)
        ssc.start()
        ssc.awaitTermination()
    }
}
```



# 4. DStream 转换



**DStream** 上的原语与 **RDD** 的类似，分为**Transformations**（转换）和**Output Operations**（输出）两种，此外转换操作中还有一些比较特殊的原语，如：**updateStateByKey()**、**transform()**以及各种**Window**相关的原语。



## 4.1 无状态操作

无状态：表示各个时间段的数据都是各自计算，比如会输出各自3s内的数据处理结果，不会使用到之前已经统计好的数据

无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化**DStream**中的每一个**RDD**。部分无状态转化操作列在了下表中。

![](https://i.bmp.ovh/imgs/2021/05/6a501379f50bf14c.png)



需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个**DStream**在内部是由许多**RDD**(批次)组成，且无状态转化操作是分别应用到每个**RDD**上的。例如，**reduceByKey()**会化简每个时间区间中的数据，但不会化简不同区间之间的数据。

举个例子，在之前的wordcount程序中，我们只会统计几秒内接收到的数据的单词个数，而不会累加。

无状态转化操作也能在多个**DStream**间整合数据，不过也是在各个时间区间内。例如，键值对**DStream**拥有和**RDD**一样的与连接相关的转化操作，也就是**cogroup()、join()、leftOuterJoin()** 等。我们可以在**DStream**上使用这些操作，这样就对每个批次分别执行了对应的RDD操作。

我们还可以像在常规的 Spark 中一样使用 **DStream**的**union()** 操作将它和另一个**DStream** 的内容合并起来，也可以使用**StreamingContext.union()**来合并多个流。



### 1）transform

**transform** 原语允许 **DStream**上执行任意的**RDD-to-RDD**函数。

可以用来执行一些 RDD 操作, 即使这些操作并没有在 SparkStreaming 中暴露出来.

该函数每一批次调度一次。其实也就是对**DStream**中的**RDD**应用转换。可以使用RDD的一些方法（Dstream没有的操作）

```scala
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}


object TransformDemo {
    def main(args: Array[String]): Unit = {
        val conf = new SparkConf().setAppName("Practice").setMaster("local[2]")
       
        val sctx = new StreamingContext(conf, Seconds(3))
        val dstream: ReceiverInputDStream[String] = sctx.socketTextStream("node1", 10000)
    
        val resultDStream = dstream.transform(rdd => {
            rdd.flatMap(_.split("\\W")).map((_, 1)).reduceByKey(_ + _)
        })
        resultDStream.print
        sctx.start
        
        sctx.awaitTermination()
    }
}
```





## 4.2 有状态操作

有状态：我们常常要的以前到现在所有数据的统计结果，那么就需要使用之前已经统计好的结果和现在进行累计。

### 1）**updateStateByKey**

**updateStateByKey**操作允许在使用新信息不断更新状态的同时能够保留他的状态.

需要做两件事情:

- 定义状态. 状态可以是任意数据类型

- 定义状态更新函数. 指定一个函数, 这个函数负责使用以前的状态和新值来更新状态.

在每个阶段, Spark 都会在所有已经存在的 **key** 上使用状态更新函数, 而不管是否有新的数据在.



```scala
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

object StreamingWordCount2 {
    def main(args: Array[String]): Unit = {
        // 设置将来访问 hdfs 的使用的用户名, 否则会出现全选不够
        System.setProperty("HADOOP_USER_NAME", "vison")
        val conf = new SparkConf().setAppName("StreamingWordCount2").setMaster("local[*]")
        // 1. 创建SparkStreaming的入口对象: StreamingContext  参数2: 表示事件间隔
        val ssc = new StreamingContext(conf, Seconds(5))
        // 2. 创建一个DStream
        val lines: ReceiverInputDStream[String] = ssc.socketTextStream("node1", 9999)
        // 3. 一个个的单词
        val words: DStream[String] = lines.flatMap(_.split("""\s+"""))
        // 4. 单词形成元组
        val wordAndOne: DStream[(String, Int)] = words.map((_, 1))
        
        
        // 开始
        /*
        1. 定义状态: 每个单词的个数就是我们需要更新的状态
        2. 状态更新函数. 每个key(word)上使用一次更新新函数
            Seq参数1: 在当前阶段 一个新的key对应的value组成的序列  在我们这个案例中是: 1,1,1,1...
            Option参数2: 上一个阶段 这个key对应的value
         */
        def updateFunction(newValue: Seq[Int], runningCount: Option[Int]): Option[Int] = {
            // 新的总数和状态进行求和操作
            val newCount: Int = (0 /: newValue) (_ + _) + runningCount.getOrElse(0)
            Some(newCount)
        }
        // 设置检查点: 使用updateStateByKey必须设置检查点
        ssc.sparkContext.setCheckpointDir("hdfs://node1:9000/checkpoint")
        val stateDS: DStream[(String, Int)] = wordAndOne.updateStateByKey[Int](updateFunction _)
        //结束
        
        //6. 显示
        stateDS.print
        //7. 启动流失任务开始计算
        ssc.start()
        //8. 等待计算结束才推出主程序
        ssc.awaitTermination()
        ssc.stop(false)
    }
}    
```

流程图：

![](https://i.bmp.ovh/imgs/2021/05/ee9c6fc105cc51a8.png)



### 2）window操作

Spark Streaming 也提供了窗口计算, 允许执行转换操作作用在一个窗口内的数据.

默认情况下, 计算只对一个时间段内的**RDD**进行, 有了窗口之后, 可以把计算应用到一个指定的窗口内的所有 RDD 上.

一个窗口可以包含多个时间段. 基于窗口的操作会在一个比**StreamingContext**的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。



比如：每6秒计算最近9s的wordcount。



![](https://i.bmp.ovh/imgs/2021/05/b52fa11a4b1f3903.png)

观察上图, 窗口在 **DStream** 上每滑动一次, 落在窗口内的那些 **RDD**会结合在一起, 然后在上面操作产生新的 RDD, 组成了 window DStream.

在上面图的情况下, 操作会至少应用在 3 个数据单元上, 每次滑动 2 个时间单位. 所以, 窗口操作需要 2 个参数:

- 1）窗口长度 – 窗口的持久时间(执行一次持续多少个时间单位)(图中是 3)

- 2）滑动步长 – 窗口操作被执行的间隔(每多少个时间单位执行一次).(图中是 2 )

**注意: 这两个参数必须是源 DStream 的 interval 的倍数.**



```scala
/*
参数1: reduce 计算规则
参数2: 窗口长度
参数3: 窗口滑动步长. 每隔这么长时间计算一次(默认是周期).
 */
val count: DStream[(String, Int)] =
wordAndOne.reduceByKeyAndWindow((x: Int, y: Int) => x + y,Seconds(15), Seconds(10))
```



```scala
/**
*有重叠数据的优化
*/
//reduceByKeyAndWindow(reduceFunc: (V, V) => V, invReduceFunc: (V, V) => V, windowDuration: Duration, slideDuration: Duration)
//（old + 新数据-要走掉的）
ssc.sparkContext.setCheckpointDir("hdfs://node1:9000/checkpoint")
val count: DStream[(String, Int)] =
    wordAndOne.reduceByKeyAndWindow((x: Int, y: Int) => x + y,(x: Int, y: Int) => x - y,Seconds(15), Seconds(10))

//
比没有invReduceFunc高效. 会利用旧值来进行计算.
invReduceFunc: (V, V) => V 窗口移动了, 上一个窗口和新的窗口会有重叠部分, 重叠部分的值可以不用重复计算了. 第一个参数就是新的值, 第二个参数是旧的值.
```



```scala
// window(windowLength, slideInterval)
基于对源 DStream 窗化的批次进行计算返回一个新的 Dstream

sourceStream
      .window(Seconds(9),Seconds(6))
      .flatMap(_.split("\\w+"))
        .map(a => (a, 1))
        .reduceByKey(_ + _)
        .print
```



# 5.DStream 输出

输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。

与**RDD**中的惰性求值类似，如果一个**DStream**及其派生出的**DStream**都没有被执行输出操作，那么这些**DStream**就都不会被求值。如果**StreamingContext**中没有设定输出操作，整个**context**就都不会启动。

```scala
//输出如下
print
saveAsTextFiles
saveAsObjectFiles
saveAsHadoopFiles
foreachRDD //用于写数据到外部存储，
....
```



注意：

- 连接不能写在driver层面（序列化）；

- 如果写在foreach则每个RDD中的每一条数据都创建，得不偿失；

- 增加foreachPartition，在分区创建（获取）。

 

```scala
//例如
stateDS.foreachRDD(rdd =>{
      rdd.foreachPartition(it =>{
        //写数据进入mysql 等其他存储器中
          可以转df直接导入数据库
      })
    })
```



# 6.DStream 编程进阶

## 6.1 累加器和广播变量

和**RDD**中的累加器和广播变量的用法完全一样. **RDD**中怎么用, 这里就怎么用.



## 6.2 DataFrame ans SQL Operations

你可以很容易地在流数据上使用 **DataFrames** 和SQL。你必须使用**SparkContext**来创建**StreamingContext**要用的**SQLContext**。

此外，这一过程可以在驱动失效后重启。我们通过创建一个实例化的**SQLContext**单实例来实现这个工作。如下例所示。我们对前例**word count**进行修改从而使用**DataFrames**和 SQL 来产生 word counts 。每个 RDD 被转换为 DataFrame，以临时表格配置并用 SQL 进行查询。

```scala
val spark = SparkSession.builder.config(conf).getOrCreate()
import spark.implicits._
count.foreachRDD(rdd =>{
    val df: DataFrame = rdd.toDF("word", "count")
    df.createOrReplaceTempView("words")
    spark.sql("select * from words").show
})
```



## 6.3 **Caching / Persistence**

和 RDDs 类似，DStreams 同样允许开发者将流数据保存在内存中。也就是说，在DStream 上使用 persist**()**方法将会自动把**DStreams**中的每个**RDD**保存在内存中。

当**DStream**中的数据要被多次计算时，这个非常有用（如在同样数据上的多次操作）。对于像**reduceByWindow**和**reduceByKeyAndWindow**以及基于状态的**(updateStateByKey)**这种操作，保存是隐含默认的。

因此，即使开发者没有调用**persist()**，由基于窗操作产生的**DStreams**会自动保存在内存中。



