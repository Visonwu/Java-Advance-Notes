

# 一、机器准备

```
准备三台机器
1.服务机器免密
	ssh
2.同步命令配置
	xcall，xsync 配置
3.时间同步器
	# timedatectl set-timezone Asia/Shanghai 
	# ntpdate ntp1.aliyun.com

```



# 二、安装软件

```shell
1.安装jdk
	在	/root/.bashrc尾中添加如下，以使 xcall jps 命令能够读取到JAVA_HOME，不使jps找不到
	source /etc/profile
		
2.编写日志生成脚本logger-collector.sh

```

```bash
# !/bin/bash
if(($#!=0)) && (($#!=2));
then 
	echo '参数只能为0个或者2个'
	exit;
fi
# 在ebusiness 2 和3执行
for i in ebusiness2 ebusiness3
do 
	# 2>&1 将错误重定向到/dev/null中 
	# $1和$2 是日志收集参数，$1表示间隔多少秒输出一条日志，$2表示输出多少条日志
	# & 后台运行 
	ssh $i java -jar /opt/module/logger-collector-0.0.1.jar  $1 $2 > /dev/null 2>&1 &
done
```



```bash
3.安装hadoop及配置文件

4.编写hadoop启动脚本

# !/bin/bash

if(($#!=1))
then
	echo 请输入参数命令
	exit;
fi

# 只能输入start和stop命令
if (($1 == start)) || (($1 == stop))
then
	$1-dfs.sh
	$1-yarn.sh
	mr-jobhistory-daemon.sh $1 historyserver
else
	echo '请输入start|stop命令'
	exit;
fi
```

```shell
5.安装kafka以及zoopkeeper集群及脚本

#!/bin/bash ---zookeeper
if(($# != 1))
then
	echo "请输入start|stop|status参数"
	exit;
fi

if (($1 == start)) || (($1 == stop)) || (($1 = status))
then
	xcall zkServer.sh $1
else
	echo '参数错误'
fi


#!/bin/bash  kafka
if(($# != 1))
then
	echo '请输出start或者stop'
fi

if (($1==start))
then 
	xcall kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
elif (($1 == stop))
then
	xcall kafka-server-stop.sh
else
	echo '参数错误'

fi
---
kafka-monitor,kafka-tools,kafka-manager监控kafka消息
```

·

```shell
6.安装flume及编写kafka的channel，consumer以及flume脚本的启停脚本


#!bin/bash
if(($# != 1))
then
        echo 请输入start|stop
        exit;
fi

# 定一个cmd作为启动命令
cmd=cmd 
if [$1 = start]
then
        cmd="flume-ng agent -c $FLUME_HOME/conf/ -n a1 -f $FLUME_HOME/test/file-flume-kafka.conf -Dflume.root.logger=INFO,console > /tmp/flume-kafaka.log 2>&1 &"

elif[$1 = stop]
then 
        cmd="ps -ef | grep flume | grep -v grep |awk '{print \$2}' | xargs kill"
fi
```



7.安装hive和tez；

创建hive的表结构，并将数据导入hive中

