​	从数据在信息系统中的生命周期看，大数据从数据源开始，经过分析、挖掘到最终获得价值一般需要经过6个主要环节￼，包括**数据收集、数据存储、资源管理与服务协调、计算引擎、数据分析和数据可视化**，技术体系如下图所示。每个环节都面临不同程度的技术挑战。



![1589171802627](C:\Users\vison\AppData\Roaming\Typora\typora-user-images\1589171802627.png)



## 1.1 大数据技术体系之 Hadoop和Spark生态

![1589171973094](C:\Users\vison\AppData\Roaming\Typora\typora-user-images\1589171973094.png)



**1．数据收集层：**
❑ 主要由关系型与非关系型数据收集组件，分布式消息队列构成。

- ❑ Sqoop￼/Canal￼：关系型数据收集和导入工具，是连接关系型数据库（比如MySQL）和Hadoop（比如HDFS）的桥梁，Sqoop可将关系型数据库中的数据全量导入Hadoop，反之亦可，而Canal则可用于实现数据的增量导入。
- ❑ Flume￼：非关系型数据收集工具，主要是流式日志数据，可近实时收集，经过滤，聚集后加载到HDFS等存储系统。
- ❑ Kafka￼：分布式消息队列，一般作为数据总线使用，它允许多个数据消费者订阅并获取感兴趣的数据。相比于其他消息队列，它采用分布式高容错设计，更适合大数据应用场景。



**2．数据存储层**
❑ 主要由分布式文件系统（面向文件的存储）和分布式数据库（面向行/列的存储）构成。

- ❑ HDFS￼:Hadoop分布式文件系统，Google GFS的开源实现，具有良好的扩展性与容错性等优点，尤其是出色的容错机制设计，使得它非常适合构建在廉价机器上，这大大降低了大数据存储成本。目前开源社区已经开发了各种类型的数据存储格式，包括SSTable（Sorted String Table）￼，文本文件、二进制key/value格式Sequence File、列式存储格式Parquet￼、ORC￼和Carbondata￼等。
- ❑ HBase￼：构建在HDFS之上的分布式数据库，Google BigTable的开源实现，允许用户存储结构化与半结构化的数据，支持行列无限扩展以及数据随机查找与删除。
- ❑ Kudu￼：分布式列式存储数据库，允许用户存储结构化数据，支持行无限扩展以及数据随机查找与更新。

**3．资源管理与服务协调**

- ❑ YARN￼：统一资源管理与调度系统，它能够管理集群中的各种资源（比如CPU和内存等），并按照一定的策略分配给上层的各类应用。YARN内置了多种多租户资源调度器，允许用户按照队列的方式组织和管理资源，且每个队列的调度机制可独立定制。
- ❑ ZooKeeper￼：基于简化的Paxos协议实现的服务协调系统，它提供了类似于文件系统的数据模型，允许用户通过简单的API实现leader选举、服务命名、分布式队列与分布式锁等复杂的分布式通用模块。

**4．计算引擎层**
❑ 包含批处理，交互式处理和流式实时处理三种引擎。

- ❑ MapReduce/Tez￼:MapReduce是一个经典的批处理计算引擎，它是Google MapReduce的开源实现，具有良好的扩展性与容错性，允许用户通过简单的API编写分布式程序；Tez是基于MapReduce开发的通用DAG（Directed Acyclic Graph的简称，有向无环图）计算引擎，能够更加高效地实现复杂的数据处理逻辑，目前被应用在Hive、Pig等数据分析系统中。
- ❑ Spark￼：通用的DAG计算引擎，它提供了基于RDD（Resilient Distributed Dataset）的数据抽象表示，允许用户充分利用内存进行快速的数据挖掘和分析。
- ❑ Impala￼/Presto￼：分别由Cloudera和Facebook开源的MPP（MassivelyParallel Processing）系统，允许用户使用标准SQL处理存储在Hadoop中的数据。它们采用了并行数据库架构，内置了查询优化器，查询下推，代码生成等优化机制，使得大数据处理效率大大提高。
- ❑ Storm￼/Spark Streaming：分布式流式实时计算引擎，具有良好的容错性与扩展性，能够高效地处理流式数据，它允许用户通过简单的API完成实时应用程序的开发工作。

**5．数据分析层**
❑ 为方便用户解决大数据问题而提供的各种数据分析工具。

- ❑ Hive￼￼/SparkSQL：在计算引擎之上构建的支持SQL或脚本语言的分析系统，大大降低了用户进行大数据分析的门槛。其中，Hive是基于MapReduce/Tez实现的SQL引擎，Pig是基于MapReduce/Tez实现的工作流引擎，SparkSQL是基于Spark实现的SQL引擎。
- ❑ Mahout￼/MLlib：在计算引擎之上构建的机器学习库实现了常用的机器学习和数据挖掘算法。其中，Mahout最初是基于MapReduce实现的，目前正逐步迁移到Spark引擎上，MLlib是基于Spark实现的。
- ❑ Apache Beam￼/Cascading￼：基于各类计算框架而封装的高级API，方便用户构建复杂的数据流水线。Apache Beam统一了批处理和流式处理两类计算框架，提供了更高级的API方便用户编写与具体计算引擎无关的逻辑代码；Cascading内置了查询计划优化器，能够自动优化用户实现的数据流。采用了面向tuple的数据模型，如果你的数据可表示成类似于数据库行的格式，则使用Cascading处理将变得很容易。



