## 1.常见的部署方案

- 滚动更新：服务不会停止，但是整个pod会有新旧并存的情况。

- 重新创建：先停止旧的pod，然后再创建新的pod，这个过程服务是会间断的。

- 蓝绿：

  ```text
  **无需停机，风险较小**
  01-部署v1的应用（一开始的状态）
  	所有外部请求的流量都打到这个版本上.
  02-部署版本2的应用
  	版本2的代码与版本1不同(新功能、Bug修复等).
  03-将流量从版本1切换到版本2。
  04-如版本2测试正常，就删除版本1正在使用的资源（例如实例），从此正式用版本2。
  ```

- 金丝雀/灰度发布

  ```text
  灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度，而我们平常所说的金丝雀部署也就是灰度发布的一种方式
  ```

### 1.1 滚动更新

```text
服务不会停止，但是整个pod会有新旧并存的情况
```



> maxSurge ：滚动升级时先启动的pod数量 
>
> maxUnavailable ：滚动升级时允许的最大unavailable的pod数量

```shell
kubectl apply -f rollingupdate.yaml
kubectl get pods
kubectl get svc
curl cluster-ip/dockerfile
```

​	rollingupdate.yaml文件如下：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rollingupdate
spec:
  strategy:
    rollingUpdate:
      maxSurge: 25%		   #滚动升级时先启动的pod数量
      maxUnavailable: 25%  # 滚动升级时允许的最大unavailable的pod数量
    type: RollingUpdate    #表示滚动更新
  selector:
    matchLabels:
      app: rollingupdate
  replicas: 4
  template:
    metadata:
      labels:
        app: rollingupdate
    spec:
      containers:
      - name: rollingupdate   #下面这个image需要自己镜像仓库的版本号管理
        image: registry.cn-hangzhou.aliyuncs.com/itcrazy2016/test-docker-image:v1.0
        ports:
        - containerPort: 8080  
---
apiVersion: v1
kind: Service
metadata:
  name: rollingupdate
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: rollingupdate
  type: ClusterIP
```

**修改**rollingupdate.yaml文件，将镜像修改成**v2.0**

```shell
# 在w1上，不断地访问观察输出 
while sleep 0.2;do curl cluster-ip/dockerfile;echo "";done
# 在w2上，监控pod 
kubectl get pods -w 
# 使得更改生效 
kubectl apply -f rollingupdate.yaml

#查看下详情
kubectl get pods
kubectl get svc
```

结论：发现新旧的POD都会存在



### 1.2 重新创建

```text
先停止旧的pod，然后再创建新的pod，这个过程服务是会间断的。
```



执行如下命令：

```shell
kubectl apply -f recreate.yaml
kubectl get pods

#修改recreate.yaml 镜像的版本号，然后重新执行，然后观察pod的状态，会发现先关闭后创建
kubectl apply -f recreate.yaml  
```

recreate.yaml文件

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recreate
spec:
  strategy: 
    type: Recreate   #表示重新创建
  selector:
    matchLabels:
      app: recreate
  replicas: 4
  template:
    metadata:
      labels:
        app: recreate
    spec:
      containers:
      - name: recreate   #主要修改下面这个镜像的版本号
        image: registry.cn-hangzhou.aliyuncs.com/itcrazy2016/test-docker-image:v1.0
        ports:
        - containerPort: 8080
        livenessProbe:
          tcpSocket:
            port: 8080
```



```shell
kubectl rollout pause deploy rollingupdate #暂停更新资源

kubectl rollout resume deploy rollingupdate  #重新启动更新资源

kubectl rollout undo deploy rollingupdate # 回到上一个版本
```



### 1.3 蓝绿

```text
**无需停机，风险较小**
01-部署v1的应用（一开始的状态）
	所有外部请求的流量都打到这个版本上.
02-部署版本2的应用
	版本2的代码与版本1不同(新功能、Bug修复等).
03-将流量从版本1切换到版本2。
04-如版本2测试正常，就删除版本1正在使用的资源（例如实例），从此正式用版本2。
```

bluegreen.yaml

```yaml
#deploy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  selector:
    matchLabels:
      app: bluegreen
  replicas: 4
  template:
    metadata:
      labels:
        app: bluegreen
        version: v1.0
    spec:
      containers:
      - name: bluegreen
        image: registry.cn-hangzhou.aliyuncs.com/itcrazy2016/test-docker-image:v1.0
        ports:
        - containerPort: 8080
```

bluegreen-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: bluegreen
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: bluegreen
    version: v1.0
  type: ClusterIP
```



**1) 执行这个命令**

```shell
kubectl apply -f bluegreen.yaml
kubectl get pods

kubectl apply -f bluegreen-service.yaml
kubectl get svc
# 在w1上不断访问观察
while sleep 0.3;do curl cluster-ip/dockerfile;echo "";done
```



**2) 现在开始修改文件做蓝绿发布**

```shell
# 修改 bluegreen.yaml

01-deployment-name:blue ---> green

02-image:v1.0---> v2.0

03-version:v1.0 ---> v2.0
```

```shell
kubectl apply -f bluegreen.yaml
kubectl get pods
# 同时观察刚才访问的地址有没有变化
可以发现，两个版本就共存了，并且之前访问的地址没有变化
```

**修改bluegreen-service.yaml**

```shell
# 也就是把流量切到2.0的版本中
selector:
  app: bluegreen
  version: v2.0
```

```shell
kubectl apply -f bluegreen-service.yaml
kubectl get svc
# 同时观察刚才访问的地址有没有变化
发现流量已经完全切到了v2.0的版本上
```

### 1.4 金丝雀发布/灰度/AB测试

```text
灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度，而我们平常所说的金丝雀部署也就是灰度发布的一种方式
```

修改上述 bluegreen-service.yaml

```yaml
selector:
  app: bluegreen
  version: v2.0 # 把version删除掉，只是根据bluegreen进行选择
```

```shell
kubectl apply -f bluegreen-service.yaml
# 同时观察刚才访问的地址有没有变化，istio中就更方便咯
此时新旧版本能够同时被访问到，AB测试，新功能部署少一些的实例
```



## 2.日志和监控

### 2.1 日志

#### 1）容器级别

- docker

  ```shell
  docker ps --->containerid
  docker logs containerid --->查看容器的日志情况
  ```

- kubectl

  ```shell
  kubectl logs -f <pod-name> -c <container-name>
  ```

#### 2) POD级别

```shell
kubectl describe pod springboot-demo-68b89b96b6-sl8bq
```



#### 3) 组件服务级别

比如kube-apiserver、kube-schedule、kubelet、kube-proxy、kube-controller-manager等
可以使用journalctl进行查看

```shell
journalctl -u kubelet
```



#### 4) 第三方日志ELK

https://github.com/AliyunContainerService/log-pilot

下面的log-pilot.yaml、elasticsearch.yaml、kibana.yaml见同级目录中

​	log-monitor文件夹中数据

**架构图：**

![Q0M441.png](https://s2.ax1x.com/2019/12/09/Q0M441.png)

- 部署logpilot

  ```shell
  #(1)部署logpilot
  kubectl apply -f log-pilot.yaml
  
  #(2)查看pod和daemonset的信息
  kubectl get pods -n kube-system
  kubectl get pods -n kube-system -o wide | grep log
  kubectl get ds -n kube-system
  ```

- 部署elasticsearch

  ```shell
  #部署es
  kubectl apply -f elasticsearch.yaml
  
  kubectl get pods -n kube-system
  kubectl get pods -n kube-system -o wide | grep ela
  
  #查看kube-system下的svc
  kubectl get svc -n kube-system
  
  #查看kube-system下的statefulset
  kubectl get statefulset -n kube-system
  ```

- 部署kibana

  kibana主要是对外提供访问的，所以这边需要配置Service和Ingress
  前提：要有Ingress Controller的支持，比如Nginx Controller

  ```shell
  #部署
  kubectl apply -f kibana.yaml
  
  #查看pod和deployment信息
  kubectl get pods -n kube-system | grep ki
  kubectl get deploy -n kube-system
  
  #配置Ingress需要的域名；打开windows上的hosts文件
  # 注意这边是worker01的IP，因为设置controller设置的w1
  121.41.10.126 kibana.jack.com
  ```

- 在windows访问kibana.jack.com



### 2.2 监控

#### 2.2.1 Prometheus简介

​    官网：https://prometheus.io/
​    github ：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus

```text
支持pull、push数据添加方式
支持k8s服务发现
提供查询语言PromQL
时序(time series)是由名字(Metric)以及一组key/value标签定义的
数据类型
```

**数据采集：**

- 通过NodeExporter：https://github.com/prometheus/node_exporter

  ![Q0tjUO.png](https://s2.ax1x.com/2019/12/09/Q0tjUO.png)

- 组件数据
  ETCD:https://ip:2379/metrics
  APIServer:https://ip:6443/metrics
  ControllerManager:https://ip:10252/metrics
  Scheduler:https://ip:10251/metrics

#### 2.2.2 Prometheus架构

   见官网



#### 2.2.3 Prometheus+Grafana安装

- (1)创建ns-monitor

  ```shell
  #(1)创建命名空间ns-monitor
  kubectl apply -f namespace.yaml
  kubectl get namespace
  ```

- (2)创建node-exporter

  ```shell
  kubectl apply -f node-exporter.yaml
  kubectl get pod -n ns-monitor
  kubectl get svc -n ns-monitor
  kubectl get ds -n ns-monitor
  # win浏览器访问集群任意一个ip，比如http://121.41.10.126:31672 查看结果 # 这边是http协议，不能用https
  ```

- (3)部署prometheus pod

  ```shell
  #包含rbac认证、ConfigMap等
  #注意：记得修改prometheus.yaml文件中的ip为master的ip和path[PV需要使用到]
  
  kubectl apply -f prometheus.yaml
  kubectl get pod -n ns-monitor
  kubectl get svc -n ns-monitor
  #win浏览器访问集群任意一个ip:30222/graph 查看结果，比如http://121.41.10.126:30137
  ```

- (4)部署grafana

  ```shell
  kubectl apply -f grafana.yaml
  kubectl get pod -n ns-monitor
  kubectl get svc -n ns-monitor
  #win浏览器访问集群任意一个ip:32405/graph/login
  #比如http://121.41.10.126:32727用户名密码:admin
  ```

- (5)增加域名访问[没有域名好像没有灵魂]；

  ```shell
  #前提：配置好ingress controller和域名解析
  kubectl apply - ingress.yaml
  kubectl get ingress -n ns-monitor
  kubectl describe ingress -n ns-monitor
  ```

- (6) 直接通过域名访问



## 3.问题解决思路

### 3.1 Master

​	master上的组件共同组成了控制平面

解决方案：出现问题时，监听到自动重启或者搭建高可用的master集群

```shell
01 若apiserver出问题了
		会导致整个K8s集群不可以使用，因为apiserver是K8s集群的大脑
02 若etcd出问题了
		apiserver和etcd则无法通信，kubelet也无法更新所在node上的状态
03 当scheduler或者controller manager出现问题时
		会导致deploy，pod，service等无法正常运行
```

### 3.2 Worker

  worker节点挂掉或者上面的kubelet服务出现问题时，w上的pod则无法正常运行。

### 3.3 Addons

   dns和网络插件比如calico发生问题时，集群内的网络无法正常通信，并且无法根据服务名称进行解析。



### 3.4 系统问题排除

- 查看Node状态

  ```shell
  kubectl get nodes
  kubectl describe node-name
  ```

- 查看集群master和worker组件的日志

  ```shell
  journalctl -u apiserver
  journalctl -u scheduler
  journalctl -u kubelet
  journalctl -u kube-proxy
  ....
  ```



### 3.5 Pod的问题排查

​    K8s中最小的操作单元是Pod，最重要的操作也是Pod，其他资源的排查可以参照Pod问题的排查

(1)查看Pod运行情况

```shell
kubectl get pods -n namespace
```

(2)查看Pod的具体描述，定位问题

```shell
kubectl describe pod pod-name -n namespace
```

(3)检查Pod对应的yaml是否有误

```shell
kubectl get pod pod-name -o yaml
```

(4)查看Pod日志

```shell
kubectl logs ...
```



**Pod可能会出现哪些问题及解决方案**

```shell
01 处于Pending状态
	说明Pod还没有被调度到某个node上，可以describe一下详情。可能因为资源不足，端口被占用等。

02 处于Waiting/ContainerCreating状态
	可能因为镜像拉取失败，或者是网络插件的问题，比如calico，或者是容器本身的问题，可以检查一下容器的yaml文件内容和Dockerfile的书写。

03 处于ImagePullBackOff状态
	镜像拉取失败，可能是镜像不存在，或者没有权限拉取。

04 处于CrashLoopBackOff状态
	Pod之前启动成功过，但是又失败了，不断在重启。

05 处于Error状态
	有些内容不存在，比如ConfigMap，PV，没有权限等，需要创建一下。

06 处于Terminating状态
	说明Pod正在停止

07 处于Unknown状态
	说明K8s已经失去对Pod的管理监听
```

